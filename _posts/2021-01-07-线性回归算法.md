---
layout:     post   				    
title:      线性回归算法				
subtitle:    
date:       2021-01-07 				
author:     WarmStar 						
header-img: img/blog17.jpg 	
catalog: true 				
tags:							
    - ML
---

> 涉及向量和矩阵的运算部分，详细可参考《线性代数》相关内容。
>
> ‘多元线性回归公式推导篇‘ 使用的部分公式来自《机器学习》(周志华)



## 简述

线性回归是以数理统计方法为基础，确定两种或多种变量间定量关系的统计分析方法。在回归分析中，如果只包含一个自变量和一个因变量，且二者的关系可近似用一条直线表示，这种回归关系被称为**一元线性回归**；如果回归分析中包含两个或多个自变量，且因变量和自变量之间是近似线性关系，则称为**多元线性回归**。

回忆一下，其实初中数学课上就学过基于最小二乘法的一元线性回归方程的解法了，那时候的题目基本就是给一个平面直角坐标系，然后一系列二维的坐标点，然后计算回归方程和偏差什么的，这也算是接触的最早的机器学习算法了。

例如`颜值 = 0.9 * 身高 - 0.2 * 体重 - 20(瞎写的)`就是一个二元线性回归公式，这里`0.9`和`-0.2`是回归系数，`-20`是偏置项，求这些参数的值的过程就是回归过程。

<br/>

## 公式推导

为了便于理解，先追忆一下初中课本学过的内容，推导一遍一元线性回归模型，再讨论更一般的多元线性回归模型。

### 一元线性回归

##### 关系式

一元线性回归方程可用如下公式表示：其中`x`是自变量，`y`是因变量，`ω`是系数，`b`是偏置项。

![CWsTs.png](https://s.im5i.com/2021/02/08/CWsTs.png)

那么此时就是求解线性模型中的`ω`和`b`两个参数，使得方程描述的数据关系与实际最相符。



##### 损失函数

损失函数用以描述实际数据和预测数据之间的偏差，显然令损失函数取到最小值的参数值就是所求的参数值。定义损失函数也有多种方法，这里以`均方误差`描述该偏差，定义损失函数为：

<img src="https://s.im5i.com/2021/02/08/CWMNq.png" alt="CWMNq.png" style="zoom:80%;" />

其中 y<sup>^</sup><sub>i</sub> 表示第`i`组数据的预测值， y<sub>i</sub> 表示第`i`组数据的实际值。`均方误差`有很好的几何意义，它对应的其实就是`欧氏距离`，所以该损失函数公式的几何意义就是**找到一条直线，使得所有样本点到该直线上的欧氏距离之和最小**。

将回归方程代入损失函数得到：

<img src="https://s.im5i.com/2021/02/08/CWTxD.png" alt="CWTxD.png" style="zoom:80%;" />

那么此时就是求得一组`ω`和`b`的值使得`L(ω, b)`取到最小值。



##### 最小二乘法

基于`均方误差最小化`来进行模型求解的方法被称为`最小二乘法`，求解一组`ω`和`b`的值使得`L(ω, b)`最小的过程称为线性回归模型的`最小二乘参数估计`。求一个函数的最值或极值的方法我们都很熟悉，那就是求导，所以这里将`L(ω, b)`分别对`ω`和`b`求导得：

<img src="https://s.im5i.com/2021/02/08/CWZqy.png" alt="CWZqy.png" style="zoom:80%;" />

令两式为 0，就可以得到最优解的解析解：

<img src="https://s.im5i.com/2021/02/08/CWh6h.png" alt="CWh6h.png" style="zoom:80%;" />

其实这个结果就是初中时期算题时就经常使用的结论，只不过当时还没有接触过求导等概念。

<br/>

### 多元线性回归

前提：对于一个向量 **x**，若无特殊说明，则默认其为`列向量`，x<sup>T</sup> 为`行向量`。

##### 关系式

对于多元线性回归方程，假设样本集容量为`m`，每组样本数据由`d`个特征描述，那么方程如下：其中x<sub>1</sub> ~ x<sub>d</sub> 为样本的`d`个特征值 ，ω<sub>1</sub> ~ ω<sub>d</sub> 为方程的`d`个回归系数，`b`为常数偏置项。

![CWcTf.jpg](https://s.im5i.com/2021/02/08/CWcTf.jpg)

转化为向量形式如下：其中 ω<sup>T</sup>为行向量，`b`为常数偏置项。

![CWkmX.jpg](https://s.im5i.com/2021/02/08/CWkmX.jpg)



##### 损失函数

为了便于描述，可以将常数偏置项并入回归系数向量得到 ω<sup>^</sup> = (ω<sup>T</sup>; b)；由于数据集容量为`m`，则可以表示为一个`m * (d + 1)`的矩阵`X`，该矩阵每行对应一个样本，每一行的前`d`个元素对应样本的`d`个特征值，最后一个元素置为`1`作为偏置项的初始值，于是得到：

![CWqeM.jpg](https://s.im5i.com/2021/02/08/CWqeM.jpg)

同样将`m`个因变量也表示为向量形式得到: **y** = (y<sub>1</sub>; y<sub>2</sub>; y<sub>3</sub>; ...; y<sub>m</sub>)，显然 **y<sup>^</sup> **= **X**ω<sup>^</sup> 。

同样地，可以得到多元线性回归方程的损失函数：

<img src="https://s.im5i.com/2021/02/09/C2Bia.jpg" alt="C2Bia.jpg" style="zoom:80%;" />

#####  求解

计算最值的方法同样是求导，那么令 E<sub>ω</sub> 对`ω`求导得到：

<img src="https://s.im5i.com/2021/02/09/CfvC7.jpg" alt="CfvC7.jpg" style="zoom:80%;" />

令上式结果为 0，就可以得到最优解的解析解：

![CJenw.jpg](https://s.im5i.com/2021/02/09/CJenw.jpg)

注意到上式结果中存在  (X<sup>T</sup>X)<sup>-1</sup> 这一项，它是 X<sup>T</sup>X 的`逆矩阵`，而矩阵可逆的充要条件是该矩阵为`满秩矩阵`，这是该公式的使用限制条件。例如当样本集的特征数目比样本集容量还大时，矩阵 **X** 的列数大于行数，X<sup>T</sup>X 显然不满秩，此时 ω<sup>^</sup> 就会有多个解。



### 广义线性模型

线性回归模型虽然简单，但也可以有多种变化。上面说的都是令预测值直接逼近真实值来得到线性回归模型的，但其实也可以令预测值逼近真实值的衍生物。例如将真实值的自然对数作为线性回归模型的逼近目标，就可以得到如下公式：

![CNTLv.jpg](https://s.im5i.com/2021/02/09/CNTLv.jpg)

这可以叫做`对数线性回归方程`，这里实际上就是令  e<sup>ω<sup>T</sup>+b</sup> 来逼近 **y** ，虽然形式上还是线性回归，但实际上是计算的是输入空间到输出空间上的非线性函数映射关系。除此之外，也可以将真实值的幂、真实值的倒数等其它更复杂的变换作为线性回归模型的逼近目标。

<br/>

## 代码实现

##### 数据集

以一个非常简单的数据集为例，先将数据绘制出来看一下。 [数据集下载地址](https://github.com/VixeruntR/MyData/blob/master/LinearRegression/linear_data.txt) 数据集共有三列：第一列的值全为 1.0，对应前述中 **X**矩阵的最后一列的偏置项初始值；第二列是数据的唯一一种的特征值 x<sub>1</sub>；第三列是目标真实值 **y**。

```python
import matplotlib.pyplot as plt

def load_dataset(file):
    x_arr = []
    y_arr = []
    with open(file) as f:
        dataset = f.readlines()
        for data in dataset:
            data = data.strip().split('\t')
            x_arr.append([float(data[0]), float(data[1])])
            y_arr.append(float(data[-1]))
    return x_arr, y_arr

def show_dataset(x_arr, y_arr):
    x_axis = []
    y_axis = []
    for i in range(len(x_arr)):
        x_axis.append(x_arr[i][1])
        y_axis.append(y_arr[i])
    figure = plt.figure()
    ax = figure.add_subplot(111)
    ax.scatter(x_axis, y_axis, s=20, alpha=0.5)
    plt.show()

if __name__ == '__main__':
    x_arr, y_arr = load_dataset('data/linear_data.txt')
    show_dataset(x_arr, y_arr)
```

![CN5ta.jpg](https://s.im5i.com/2021/02/09/CN5ta.jpg)

通过绘制出的图像可以看出，数据点的走势整体上呈上升趋势，局部区域波动较大。



##### 线性回归拟合

有了数据之后，接下来直接进行线性拟合，并将数据点和拟合直线绘制在一张图像上做对比。

```python
import matplotlib.pyplot as plt
import numpy as np

def show_dataset(x_arr, y_arr, w_):
    x_mat = np.mat(x_arr)
    x_mat.sort(0)
    y_pred = x_mat * w_
    figure = plt.figure()
    ax = figure.add_subplot(111)
    ax.plot(x_mat[:, 1], y_pred, c='red')
    ax.scatter(list(np.array(x_arr)[:, 1]),
               y_arr, s=20, alpha=0.5)
    plt.show()

def LinearRegression(X, Y):
    X = np.mat(X)
    Y = np.mat(Y).T
    XtX = X.T * X
    if np.linalg.det(XtX) != 0.0:
        return XtX.I * (X.T * Y)
    else:
        print('Singular Matrix cannot be calculated !')
        return

if __name__ == '__main__':
    x_arr, y_arr = load_dataset('data/linear_data.txt')
    w_ = LinearRegression(x_arr, y_arr)
    print(w_)
    show_dataset(x_arr, y_arr, w_)
----------------------------------------------------------------------
> [[3.00774324]
>  [1.69532264]]
```

![Coalw.jpg](https://s.im5i.com/2021/02/09/Coalw.jpg)



##### 评估

得到拟合直线之后，接下来需要评估拟合效果的好坏了，二维的数据尚且可以通过绘图来大致观察拟合情况，但更高维的数据就不行了，为此可以使用`corrcoef`方法评估拟合效果。

```python
if __name__ == '__main__':
    x_arr, y_arr = load_dataset('data/linear_data.txt')
    w_ = LinearRegression(x_arr, y_arr)
    x_mat = np.mat(x_arr)
    y_mat = np.mat(y_arr)
    y_pred = x_mat * w_
    print(np.corrcoef(y_pred.T, y_mat))
----------------------------------------------------------------------
> [[1.         0.98647356]
>  [0.98647356 1.        ]]
```

`np.corrcoef` 用以计算矩阵的相关系数，返回一个由`Pearson乘积矩相关系数`组成的矩阵。当输入两个矩阵时，结果矩阵中副对角线上的值表示这两个矩阵的相关系数；主对角线上的值表示输入矩阵与它自己的相关系数，自然就是 1.0了。

除了用矩阵相关系数描述拟合情况的好坏之外，也可以直接计算均方误差，毕竟线性回归模型的优化目标就是使均方误差最小：

```python
if __name__ == '__main__':
    x_arr, y_arr = load_dataset('data/linear_data.txt')
    w_ = LinearRegression(x_arr, y_arr)
    x_mat = np.mat(x_arr)
    y_mat = np.mat(y_arr)
    y_pred = x_mat * w_
    mse = np.mean(np.square(y_pred.T - y_mat))
    print(mse)
----------------------------------------------------------------------
> 0.006776245408407456
```

显然这里的输出结果值越小说明拟合效果越好，反之亦然。



##### 局部加权线性回归

观察上面的拟合结果绘图，可以看到虽然整体趋势是很符合的，但对于波动幅度较大的部分数据来说，偏差还是比较大的。因为线性回归求的是具有最小均方误差的无偏估计，所以可能出现**欠拟合**的情况，而如果允许在估计过程中引入一些偏差，就可以降低预测的误差。这一点可以通过`局部加权线性回归(Locally Weighted Linear Regression, LWLR)`方法实现。

局部加权线性回归是一种`非参数算法`，与 KNN算法类似，该方法没有训练过程，而是直接使用训练集中的数据来做预测。并且在预测时会更多地参考距离预测样本近的训练样本，更少地参考距离预测样本远的训练样本。

实现过程也很简单，就是将损失函数的每一项 (数据集容量为**M**时，损失函数就是**M**项加和) 乘以一个权重系数 w<sub>i</sub>，这么做的目的就是使不同的数据对损失函数产生不同程度的贡献，从而使损失函数可以取到更小的值，即达到更好的拟合效果，系数值计算公式如下：

![CvhOq.jpg](https://s.im5i.com/2021/02/10/CvhOq.jpg)

其实这个就是`高斯核函数`，



 