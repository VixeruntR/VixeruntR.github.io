---
layout:     post   				    
title:      线性回归算法				
subtitle:    
date:       2021-01-07 				
author:     WarmStar 						
header-img: img/blog17.jpg 	
catalog: true 				
tags:							
    - ML
---

> 涉及向量和矩阵的运算部分，详细可参考《线性代数》相关内容。
>
> ‘多元线性回归公式推导篇‘ 使用的部分公式来自《机器学习》(周志华)



## 简述

线性回归是以数理统计方法为基础，确定两种或多种变量间定量关系的统计分析方法。在回归分析中，如果只包含一个自变量和一个因变量，且二者的关系可近似用一条直线表示，这种回归关系被称为**一元线性回归**；如果回归分析中包含两个或多个自变量，且因变量和自变量之间是近似线性关系，则称为**多元线性回归**。

回忆一下，其实初中数学课上就学过基于最小二乘法的一元线性回归方程的解法了，那时候的题目基本就是给一个平面直角坐标系，然后一系列二维的坐标点，然后计算回归方程和偏差什么的，这也算是接触的最早的机器学习算法了。

例如`颜值 = 0.9 * 身高 - 0.2 * 体重 - 20(瞎写的)`就是一个二元线性回归公式，这里`0.9`和`-0.2`是回归系数，`-20`是偏置项，求这些参数的值的过程就是回归过程。

<br/>

## 公式推导

为了便于理解，先追忆一下初中课本学过的内容，推导一遍一元线性回归模型，再讨论更一般的多元线性回归模型。

### 一元线性回归

##### 关系式

一元线性回归方程可用如下公式表示：其中`x`是自变量，`y`是因变量，`w`是系数，`b`是偏置项。

![CWsTs.png](https://s.im5i.com/2021/02/08/CWsTs.png)

那么此时就是求解线性模型中的`w`和`b`两个参数，使得方程描述的数据关系与实际最相符。



##### 损失函数

损失函数用以描述实际数据和预测数据之间的偏差，显然令损失函数取到最小值的参数值就是所求的参数值。定义损失函数也有多种方法，这里以`均方误差`描述该偏差，定义损失函数为：

<img src="https://s.im5i.com/2021/02/08/CWMNq.png" alt="CWMNq.png" style="zoom:80%;" />

其中 y<sup>^</sup><sub>i</sub> 表示第`i`组数据的预测值， y<sub>i</sub> 表示第`i`组数据的实际值。`均方误差`有很好的几何意义，它对应的其实就是`欧氏距离`，所以该损失函数公式的几何意义就是**找到一条直线，使得所有样本点到该直线上的欧氏距离之和最小**。

将回归方程代入损失函数得到：

<img src="https://s.im5i.com/2021/02/08/CWTxD.png" alt="CWTxD.png" style="zoom:80%;" />

那么此时就是求得一组`w`和`b`的值使得`L(w, b)`取到最小值。



##### 最小二乘法

基于`均方误差最小化`来进行模型求解的方法被称为`最小二乘法`，求解一组`w`和`b`的值使得`L(w, b)`最小的过程称为线性回归模型的`最小二乘参数估计`。求一个函数的最值或极值的方法我们都很熟悉，那就是求导，所以这里将`L(w, b)`分别对`w`和`b`求导得：

<img src="https://s.im5i.com/2021/02/08/CWZqy.png" alt="CWZqy.png" style="zoom:80%;" />

令两式为 0，就可以得到最优解的解析解：

<img src="https://s.im5i.com/2021/02/08/CWh6h.png" alt="CWh6h.png" style="zoom:80%;" />

其实这个结果就是初中时期算题时就经常使用的结论，只不过当时还没有接触过求导等概念。

<br/>

### 多元线性回归

前提：对于一个向量 **x**，若无特殊说明，则默认其为`列向量`，x<sup>T</sup> 为`行向量`。

##### 关系式

对于多元线性回归方程，假设样本集容量为`m`，每组样本数据由`d`个特征描述，那么方程如下：其中x<sub>1</sub> ~ x<sub>d</sub> 为样本的`d`个特征值 ，w<sub>1</sub> ~ w<sub>d</sub> 为方程的`d`个回归系数，`b`为常数偏置项。

![CWcTf.jpg](https://s.im5i.com/2021/02/08/CWcTf.jpg)

转化为向量形式如下：其中w<sup>T</sup>为行向量，`b`为常数偏置项。

![CWkmX.jpg](https://s.im5i.com/2021/02/08/CWkmX.jpg)



##### 损失函数

为了便于描述，可以将常数偏置项并入回归系数向量得到 w<sup>^</sup> = (w<sup>T</sup>; b)；由于数据集容量为`m`，则可以表示为一个`m * (d + 1)`的矩阵`X`，该矩阵每行对应一个样本，每一行的前`d`个元素对应样本的`d`个特征值，最后一个元素置为 1，得到：

![CWqeM.jpg](https://s.im5i.com/2021/02/08/CWqeM.jpg)

同样将`m`个因变量也表示为向量形式得到: **y** = (y<sub>1</sub>; y<sub>2</sub>; y<sub>3</sub>; ...; y<sub>m</sub>)，显然 **y<sup>^</sup> **= **X**w<sup>^</sup> 。

同样地，可以得到多元线性回归方程的损失函数：

<img src="https://s.im5i.com/2021/02/09/C2Bia.jpg" alt="C2Bia.jpg" style="zoom:80%;" />

#####  求解

计算最值的方法同样是求导，那么令 E<sub>w</sub> 对`w`求导得到：

<img src="https://s.im5i.com/2021/02/09/CfvC7.jpg" alt="CfvC7.jpg" style="zoom:80%;" />

令上式结果为 0，就可以得到最优解的解析解：

![CJenw.jpg](https://s.im5i.com/2021/02/09/CJenw.jpg)

注意到上式结果中存在  (X<sup>T</sup>X)<sup>-1</sup> 这一项，它是 X<sup>T</sup>X 的`逆矩阵`，而矩阵可逆的充要条件是该矩阵为`满秩矩阵`，这是该公式的使用限制条件。例如当样本集的特征数目比样本集容量还大时，矩阵 **X** 的列数大于行数，X<sup>T</sup>X 显然不满秩，此时 w<sup>^</sup> 就会有多个解。



### 广义线性模型

线性回归模型虽然简单，但也可以有多种变化。上面说的都是令预测值直接逼近真实值来得到线性回归模型的，但其实也可以令预测值逼近真实值的衍生物。例如将真实值的自然对数作为线性回归模型的逼近目标，就可以得到如下公式：

![CNTLv.jpg](https://s.im5i.com/2021/02/09/CNTLv.jpg)

这可以叫做`对数线性回归方程`，这里实际上就是令  e<sup>w<sup>T</sup>+b</sup> 来逼近 **y** ，虽然形式上还是线性回归，但实际上是计算的是输入空间到输出空间上的非线性函数映射关系。除此之外，也可以将真实值的幂、真实值的倒数等其它更复杂的变换作为线性回归模型的逼近目标。

<br/>

## 代码实现

##### 数据集

以一个非常简单的数据集为例，先将数据绘制出来看一下。 [数据集下载地址](https://github.com/VixeruntR/MyData/blob/master/LinearRegression/linear_data.txt) 

```python
import matplotlib.pyplot as plt

def load_dataset(file):
    x_arr = []
    y_arr = []
    with open(file) as f:
        dataset = f.readlines()
        for data in dataset:
            data = data.strip().split('\t')
            x_arr.append([float(data[0]), float(data[1])])
            y_arr.append(float(data[-1]))
    return x_arr, y_arr

def show_dataset():
    x_arr, y_arr = load_dataset('data/linear_data.txt')
    x_axis = []
    y_axis = []
    for i in range(len(x_arr)):
        x_axis.append(x_arr[i][1])
        y_axis.append(y_arr[i])
    figure = plt.figure()
    ax = figure.add_subplot(111)
    ax.scatter(x_axis, y_axis, s=20, alpha=0.5)
    plt.show()

if __name__ == '__main__':
    show_dataset()
```