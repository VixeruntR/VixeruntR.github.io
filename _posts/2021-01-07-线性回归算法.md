---
layout:     post   				    
title:      线性回归算法				
subtitle:    
date:       2021-01-07 				
author:     WarmStar 						
header-img: img/blog17.jpg 	
catalog: true 				
tags:							
    - ML
---

## 简述

线性回归是以数理统计方法为基础，确定两种或多种变量间定量关系的统计分析方法。在回归分析中，如果只包含一个自变量和一个因变量，且二者的关系可近似用一条直线表示，这种回归关系被称为**一元线性回归**；如果回归分析中包含两个或多个自变量，且因变量和自变量之间是近似线性关系，则称为**多元线性回归**。

回忆一下，其实初中数学课上就学过基于最小二乘法的一元线性回归方程的解法了，那时候的题目基本就是给一个平面直角坐标系，然后一系列二维的坐标点，然后计算回归方程和偏差什么的，这也算是接触的最早的机器学习算法了。

例如`颜值 = 0.9 * 身高 - 0.2 * 体重 - 20(瞎写的)`就是一个二元线性回归公式，这里`0.9`和`-0.2`是回归系数，`-20`是偏置项，求这些参数的值的过程就是回归过程。

<br/>

## 公式推导

为了便于理解，先追忆一下初中课本学过的内容，推导一遍一元线性回归模型，再讨论更一般的多元线性回归模型。

### 一元线性回归

##### 关系式

一元线性回归方程可用如下公式表示：其中`x`是自变量，`y`是因变量，`w`是系数，`b`是偏置项。

![CWsTs.png](https://s.im5i.com/2021/02/08/CWsTs.png)

那么此时就是求解线性模型中的`w`和`b`两个参数，使得方程描述的数据关系与实际最相符。



##### 损失函数

损失函数用以描述实际数据和预测数据之间的偏差，显然令损失函数取到最小值的参数值就是所求的参数值。定义损失函数也有多种方法，这里以`均方误差`描述该偏差，定义损失函数为：

![CWnSo.png](https://s.im5i.com/2021/02/08/CWnSo.png)

其中 y<sup>^</sup><sub>i</sub> 表示第`i`组数据的预测值， y<sub>i</sub> 表示第`i`组数据的实际值。`均方误差`有很好的几何意义，它对应的其实就是`欧氏距离`，所以该损失函数公式的几何意义就是**找到一条直线，使得所有样本点到该直线上的欧氏距离之和最小**。

将回归方程代入损失函数得到：

![CWpuW.png](https://s.im5i.com/2021/02/08/CWpuW.png)

那么此时就是求得一组`w`和`b`的值使得`L(w, b)`取到最小值。



##### 最小二乘法

基于`均方误差最小化`来进行模型求解的方法被称为`最小二乘法`，求解一组`w`和`b`的值使得`L(w, b)`最小的过程称为线性回归模型的`最小二乘参数估计`。求一个函数的最值和极值的方法我们都很熟悉，那就是求导，所以这里将`L(w, b)`分别对`w`和`b`求导得：

![CW0px.png](https://s.im5i.com/2021/02/08/CW0px.png)

令两式为 0，就可以得到最优解的解析解：

![CWEGQ.png](https://s.im5i.com/2021/02/08/CWEGQ.png)

其实这个结果就是初中时期算题时就经常使用的结论，只不过当时还没有接触过求导等概念。

<br/>

### 多元线性回归

##### 关系式