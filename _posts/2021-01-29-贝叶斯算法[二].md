---
layout:     post   				    
title:      朴素贝叶斯算法[二] 				
subtitle:    
date:       2021-01-29 				
author:     WarmStar 						
header-img: img/blog16.jpg 	
catalog: true 				
tags:							
    - ML
---

### 垃圾邮件分类

介绍完了朴素贝叶斯算法的原理和代码实现，这里实现一个经典的垃圾邮件分类任务。数据集来自网络，原始数据集中有两处单词乱码，已修正。[下载地址](https://github.com/VixeruntR/MyData/blob/master/NaiveBayesian/email.rar) 

##### 数据集

+ `get_dataset`函数用以读取数据集中的`txt`文件，并将`spam`文件夹的内容定义为`类别1`，将`ham`文件夹的内容定义为`类别0`
+ `mail2word`函数用以将每个`txt`文件里的内容切分成单独的单词并以列表形式返回
+ `create_glossary`函数生成词汇表

```python
import re
import os

def mail2word(mail_content):
    # 以任意非字母或非数字切分整个邮件内容
    
    words = re.split(r'\W+', mail_content)
    # 仅保留长度大于2且不含数字的单词 并全部转化为小写
    
    words = [word.lower() for word in words if
             len(word) > 2 and not bool(re.search(r'\d', word))]
    return words

def create_glossary(words_list):
    glossary = set([])
    for words in words_list:
        glossary = glossary | set(words)
    return list(glossary)

def get_dataset(root):
    word_list = []
    label_list = []
    spam_root = os.path.join(root, 'spam')
    for spam_file in os.listdir(spam_root):
        spam_file_path = os.path.join(spam_root, spam_file)
        words = mail2word(open(spam_file_path, 'r').read())
        word_list.append(words)
        label_list.append(1)

    ham_root = os.path.join(root, 'ham')
    for ham_file in os.listdir(ham_root):
        ham_file_path = os.path.join(ham_root, ham_file)
        words = mail2word(open(ham_file_path, 'r').read())
        word_list.append(words)
        label_list.append(0)
    return word_list, label_list

if __name__ == '__main__':
    word_list, label_list = get_dataset('dataset/email/')
    glossary = create_glossary(word_list)
```

##### 训练和测试

`cut_dataset_index`函数用以将数据集划分为训练集和测试集，划分依据是先根据数据集总容量`N`产生`0 ~ N-1`的索引列表，再随机打乱索引列表，取前80%作为训练集，后20%作为测试集。这里没有给定随机种子，所以每次运行得到的结果不同。

```python
def cut_dataset_index(N):
    all_index = list(range(N))
    random.shuffle(all_index)
    train_index = all_index[0:N * 4 // 5]
    test_index = all_index[N * 4 // 5:]
    return train_index, test_index

if __name__ == '__main__':
    word_list, label_list = get_dataset('data/email/')
    glossary = create_glossary(word_list)
    train_index, test_index = cut_dataset_index(len(label_list))

    train_matrix = []
    train_labels = []
    for train_i in train_index:
        train_matrix.append(words2vector(glossary, word_list[train_i]))
        train_labels.append(label_list[train_i])
    p0_vector, p1_vector, p1_prob = NaiveBayesian(
        np.array(train_matrix), np.array(train_labels))

    errorCount = 0
    for test_i in test_index:
        words_vector = words2vector(glossary, word_list[test_i])
        if predict(np.array(words_vector),
                   p0_vector, p1_vector, p1_prob) != label_list[test_i]:
            errorCount += 1
    print('错误率: %.2f%%' % (float(errorCount) / len(test_index) * 100))
```

<br/>

### sklearn实现朴素贝叶斯

##### API 简介

`sklearn`中封装了3种朴素贝叶斯分类算法，分别是`GaussianNB`，`MultinomialNB`和`BernoulliNB`：

+ GaussianNB：先验为高斯分布的朴素贝叶斯，适用于样本特征分布是连续值的情况
+ MultinomialNB：就是先验为多项式分布的朴素贝叶斯，适用于样本特征分布是多元离散值的情况
+ BernoulliNB：就是先验为伯努利分布的朴素贝叶斯，适用于样本特征分布是二元离散值或者很稀疏的多元离散值的情况



