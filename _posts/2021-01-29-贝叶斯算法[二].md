---
layout:     post   				    
title:      朴素贝叶斯算法[二] 				
subtitle:    
date:       2021-01-29 				
author:     WarmStar 						
header-img: img/blog16.jpg 	
catalog: true 				
tags:							
    - ML
---

### 垃圾邮件分类

介绍完了朴素贝叶斯算法的原理和代码实现，这里实现一个经典的垃圾邮件分类任务。数据集来自网络，原始数据集中有两处单词乱码，已修正。[下载地址](https://github.com/VixeruntR/MyData/blob/master/NaiveBayesian/email.rar) 

##### 数据集

+ `get_dataset`函数用以读取数据集中的`txt`文件，并将`spam`文件夹的内容定义为`类别1`，将`ham`文件夹的内容定义为`类别0`
+ `mail2word`函数用以将每个`txt`文件里的内容切分成单独的单词并以列表形式返回

```python
import re
import os

def mail2word(mail_content):
    # 以任意非字母或非数字切分整个邮件内容
    
    words = re.split(r'\W+', mail_content)
    # 仅保留长度大于2且不含数字的单词 并全部转化为小写
    
    words = [word.lower() for word in words if
             len(word) > 2 and not bool(re.search(r'\d', word))]
    return words

def create_glossary(words_list):
    glossary = set([])
    for words in words_list:
        glossary = glossary | set(words)
    return list(glossary)

def get_dataset(root):
    word_list = []
    label_list = []
    spam_root = os.path.join(root, 'spam')
    for spam_file in os.listdir(spam_root):
        spam_file_path = os.path.join(spam_root, spam_file)
        words = mail2word(open(spam_file_path, 'r').read())
        word_list.append(words)
        label_list.append(1)

    ham_root = os.path.join(root, 'ham')
    for ham_file in os.listdir(ham_root):
        ham_file_path = os.path.join(ham_root, ham_file)
        words = mail2word(open(ham_file_path, 'r').read())
        word_list.append(words)
        label_list.append(0)
    return word_list, label_list

if __name__ == '__main__':
    word_list, label_list = get_dataset('data/email/')
    glossary = create_glossary(word_list)
```

##### 训练和测试

```python
if __name__ == '__main__':
    word_list, label_list = get_dataset('data/email/')
    glossary = create_glossary(word_list)

    datasetNum = len(label_list)
    all_index = list(range(datasetNum))
    random.shuffle(all_index)
    train_index = all_index[0:datasetNum * 4 // 5]
    test_index = all_index[datasetNum * 4 // 5:]

    train_matrix = []
    train_labels = []
    for train_i in train_index:
        train_matrix.append(words2vector(glossary, word_list[train_i]))
        train_labels.append(label_list[train_i])
    p0_vector, p1_vector, p1_prob = NaiveBayesian(
        np.array(train_matrix), np.array(train_labels))

    errorCount = 0
    for test_i in test_index:
        words_vector = words2vector(glossary, word_list[test_i])
        if predict(np.array(words_vector), 
                   p0_vector, p1_vector, p1_prob) != label_list[test_i]:
            errorCount += 1
    print('错误率: %.2f%%' % (float(errorCount) / len(test_index) * 100))
```