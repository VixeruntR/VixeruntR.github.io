---
layout:     post   				    
title:      朴素贝叶斯算法[二] 				
subtitle:    
date:       2021-01-29 				
author:     WarmStar 						
header-img: img/blog16.jpg 	
catalog: true 				
tags:							
    - ML
---

## 垃圾邮件分类

介绍完了朴素贝叶斯算法的原理和代码实现，这里实现一个经典的垃圾邮件分类任务。数据集来自网络，原始数据集中有两处单词乱码，已修正。[下载地址](https://github.com/VixeruntR/MyData/blob/master/NaiveBayesian/email.rar) 

##### 数据集

+ `get_dataset`函数用以读取数据集中的`txt`文件，并将`spam`文件夹的内容定义为`类别1`，将`ham`文件夹的内容定义为`类别0`
+ `mail2word`函数用以将每个`txt`文件里的内容切分成单独的单词并以列表形式返回
+ `create_glossary`函数生成词汇表

```python
import re
import os

def mail2word(mail_content):
    # 以任意非字母或非数字切分整个邮件内容
    
    words = re.split(r'\W+', mail_content)
    # 仅保留长度大于2且不含数字的单词 并全部转化为小写
    
    words = [word.lower() for word in words if
             len(word) > 2 and not bool(re.search(r'\d', word))]
    return words

def create_glossary(words_list):
    glossary = set([])
    for words in words_list:
        glossary = glossary | set(words)
    return list(glossary)

def get_dataset(root):
    word_list = []
    label_list = []
    spam_root = os.path.join(root, 'spam')
    for spam_file in os.listdir(spam_root):
        spam_file_path = os.path.join(spam_root, spam_file)
        words = mail2word(open(spam_file_path, 'r').read())
        word_list.append(words)
        label_list.append(1)

    ham_root = os.path.join(root, 'ham')
    for ham_file in os.listdir(ham_root):
        ham_file_path = os.path.join(ham_root, ham_file)
        words = mail2word(open(ham_file_path, 'r').read())
        word_list.append(words)
        label_list.append(0)
    return word_list, label_list

if __name__ == '__main__':
    word_list, label_list = get_dataset('dataset/email/')
    glossary = create_glossary(word_list)
```

##### 训练和测试

`cut_dataset_index`函数用以将数据集划分为训练集和测试集，划分依据是先根据数据集总容量`N`产生`0 ~ N-1`的索引列表，再随机打乱索引列表，取前80%作为训练集，后20%作为测试集。这里没有给定随机种子，所以每次运行得到的结果不同。

```python
def cut_dataset_index(N):
    all_index = list(range(N))
    random.shuffle(all_index)
    train_index = all_index[0:N * 4 // 5]
    test_index = all_index[N * 4 // 5:]
    return train_index, test_index

if __name__ == '__main__':
    word_list, label_list = get_dataset('data/email/')
    glossary = create_glossary(word_list)
    train_index, test_index = cut_dataset_index(len(label_list))

    train_matrix = []
    train_labels = []
    for train_i in train_index:
        train_matrix.append(words2vector(glossary, word_list[train_i]))
        train_labels.append(label_list[train_i])
    p0_vector, p1_vector, p1_prob = NaiveBayesian(
        np.array(train_matrix), np.array(train_labels))

    errorCount = 0
    for test_i in test_index:
        words_vector = words2vector(glossary, word_list[test_i])
        if predict(np.array(words_vector),
                   p0_vector, p1_vector, p1_prob) != label_list[test_i]:
            errorCount += 1
    print('错误率: %.2f%%' % (float(errorCount) / len(test_index) * 100))
```

<br/>

## sklearn实现朴素贝叶斯

### API 简介

`sklearn`中封装了3种朴素贝叶斯分类算法，分别是`MultinomialNB`，`GaussianNB`和`BernoulliNB`，可通过`from sklearn.naive_bayes import *`导入，下面一一介绍。

##### MultinomialNB

先验为**多项式分布**的朴素贝叶斯，适用于样本特征分布是**多元离散值**的情况，公式即为 [朴素贝叶斯算法[一]](https://vixeruntr.github.io/2021/01/20/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95-%E4%B8%80/#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0) 中推导的条件概率公式(这里加上了λ)：

![k63nS.png](https://s.im5i.com/2021/02/01/k63nS.png)

`P(Xj = Xjl|Y = Ck)`表示数据集第`k`个类别的第`j`维特征的第`l`个取值的条件概率，m<sub>k</sub> 是数据集中第`k`个类别的子集容量。该函数的参数如下：

+ **alpha**：浮点数，默认为`1.0`，对应公式中的拉普拉斯平滑参数`λ`，设置为`0`则不添加平滑。
+ **fit_prior**：布尔值，该参数表示是否考虑先验概率，默认为`True`，此时先验概率的值由第三个参数`class_prior`决定。如果设置为`false`，则考虑所有类别有相同的先验概率，`class_prior`参数不再生效。
+ **class_prior**：预先输入的先验概率，默认为`None`，此时`MultinomialNB`自动从训练集计算先验概率，此时的先验概率为 P(Y=C<sub>k</sub>) = m<sub>k</sub>/m。其中`m`是训练集总容量，m<sub>k</sub> 是输出为第`k`个类别的子集容量。

为更明确地表述参数间关系，总结为如下表格：

| fit_prior | class_prior |               先验概率               |
| :-------: | :---------: | :----------------------------------: |
|   False   |   不生效    |       P(Y=C<sub>k</sub>) = 1/k       |
|   True    |    None     | P(Y=C<sub>k</sub>) = m<sub>k</sub>/m |
|   True    | class_prior |   P(Y=C<sub>k</sub>) = class_prior   |



##### GaussianNB

先验为**高斯分布**的朴素贝叶斯，适用于样本特征分布是**连续值**的情况，公式如下：

![kOIao.png](https://s.im5i.com/2021/02/01/kOIao.png)

该函数的参数如下：

+ **priors**：预先输入的先验概率，默认为`None`，此时先验概率 P(Y=C<sub>k</sub>) = m<sub>k</sub>/m ，否则先验概率以输入值为准。
+ **var_smoothing**：浮点值，默认为`1e-9`。设置为特征值之间的最小差值，保证计算稳定性。



##### BernoulliNB

先验为**伯努利分布**的朴素贝叶斯，适用于样本特征分布是**二元离散值**的情况，公式如下：![kouhF.png](https://s.im5i.com/2021/02/02/kouhF.png)

该算法的函数有4个参数(其中三个函数与`MultinomialNB`完全一致，不再赘述)：

+ **binarize**：可输入浮点数或`None`，默认为`0.0`。如果参数值为`None`，则算法认为每种数据特征都是二元的。否则将小于等于`binarize`值的归为一类，大于`binarize`值的归为另一类。



##### 预测函数

上述三种朴素贝叶斯方法的唯一区别就在于条件概率公式的不同，如何选用取决于数据特征分布的情况。预测时`sklearn`一共封装了三种函数：

+ predict：最常规的预测函数，直接返回测试样本的预测类别结果。
+ predict_proba：返回测试样本在各个类别上的预测概率，则这些概率值里的最大值对应的类别就是`predict`方法得到的类别。
+ predict_log_proba：比`predict_proba`多了一步将预测概率值进行对数转化的操作。



### 数据集

```python
import os
import random
import jieba

def is_all_chinese(word):
    for w in word:
        if not '\u4e00' <= w <= '\u9fa5':
            return False
    return True

def filter_words(word_list):
    words = []
    for word in list(word_list):
        if is_all_chinese(word):
            words.append(word)
    return words

def calc_words_frequency(dataset):
    words_frequency = {}
    for word_list in dataset:
        for word in word_list:
            if word in words_frequency.keys():
                words_frequency[word] += 1
            else:
                words_frequency[word] = 1

    words_frequency = sorted(words_frequency.items(),
                             key=lambda x: x[1], reverse=True)
    words_after_sort, _ = zip(*words_frequency)
    return list(words_after_sort)


def load_dataset(root, test_size=0.2):
    data_list = []
    label_list = []

    for folder in os.listdir(root):
        folder_path = os.path.join(root, folder)
        for txt in os.listdir(folder_path):
            with open(os.path.join(folder_path, txt), 'r', 
                      encoding='utf-8') as f:
                data = f.read()
            words = jieba.cut(data, cut_all=False)
            words = filter_words(words)
            data_list.append(words)
            label_list.append(folder)

    data_label_list = list(zip(data_list, label_list))
    random.shuffle(data_label_list)
    index = int(len(data_label_list) * test_size) + 1
    train_list = data_label_list[index:]
    test_list = data_label_list[:index]

    train_dataset, train_labels = zip(*train_list)
    test_dataset, test_labels = zip(*test_list)
    words_after_sort = calc_words_frequency(train_dataset)
    return words_after_sort, train_dataset, test_dataset, train_labels, test_labels

if __name__ == '__main__':
    root = 'data/NEWS/Sample'
    words_after_sort, train_dataset, test_dataset, train_labels, test_labels = load_dataset(root, test_size=0.2)
```