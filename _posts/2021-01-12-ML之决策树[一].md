---
layout:     post   				    
title:      决策树算法[一] 				
subtitle:    
date:       2021-01-12 				
author:     WarmStar 						
header-img: img/blog13.jpg 	
catalog: true 				
tags:							
    - ML
---

> 本文介绍的有关“熵”的概念，如需详细了解请阅读《信息论与编码》等书。
>
> 概率和条件概率等概念，如需详细了解请阅读《概率论与数理统计》等书。



## 简述

**决策树(decision tree)**是一种基本的分类与回归方法，基于树结构进行决策的，可以看作`if-then`规则的集合。一棵决策树包含**一个根节点**、**若干内部节点**和**若干叶子节点**。其中根节点包含所有样本点，内部节点作为决策节点(属性测试)，叶子节点对应决策结果。



##### 重要概念

+ 根节点(Root Node)：表示整个样本集合，并且该节点可以进一步划分成两个或多个子集。
+ 决策节点(Decision Node)：当一个子节点可以被进一步拆分成多个子节点时，这个子节点就叫做决策节点。
+ 叶子节点(Leaf/Terminal Node)：无法再拆分的节点被称为叶子节点。
+ 拆分(Splitting)：表示将一个节点拆分成多个子集的过程。
+ 剪枝(Pruning)：移除决策树中无用子节点的过程就叫做剪枝，与拆分过程相反。
+ 分支/子树(Branch/Sub-Tree)：决策树的某一部分叫做分支或子树。



##### 决策原理

先由决策树的`根节点(root node)`到`叶子节点(leaf node)`的每一条路径构建一条规则，路径上决策节点的特征对应着规则的条件，叶子节点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：***互斥且完备***，即每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。实际用决策树进行分类是从根节点开始，对实例的某一特征进行测试，根据测试结果将实例分配到其子节点，若该子节点仍为决策节点，则继续进行判断与分配，直至将实例分到叶子节点的类中。



##### 实例

看下面一个决策树实例，数据来自 [ProcessOn公开数据](https://www.processon.com/view/57368ab8e4b0d4e09772f2a5?fromnew=1)

![dectree1.jpg](https://s.im5i.com/2021/01/12/dectree1.jpg)

这是一个根据天气状况决定是否出行的决策过程：如果是阴天直接选择出行，如果是晴天再根据空气湿度判断是否出行，如果是雨天再根据是否有风判断是否出行。其实生活中决策树算法的思想很常用，例如买衣服时，看到一款衣服决定是否购买要经过价格、品牌、款式和舒适度等条件的判断对比，脑海中纠结的过程其实就是一棵决策树。



##### 算法流程

构建一个决策树算法的主要步骤大致如下：

- 收集数据：使用任何方法收集整理所需数据。
- 准备数据：将收集的数据按照一定规则整理和存储。
- 分析数据：对数据进行特征筛选，数据变换、清洗等操作。
- 训练算法：即构造决策树，也可以叫做决策树学习。
- 测试算法：使用训练好的决策树计算准确率。
- 优化算法：根据测得的准确率对数据或算法进行优化。

<br/>

## 信息增益

假设现在已经有了一份整理好的数据集，那么在训练决策树之前，需要对数据集进行分析、清洗变换和特征选择等操作，其中特征选择是最重要的一环。

##### 特征选择

特征选择是为了选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。例如挑选衣服时，主要关注的是价格、舒适度等特征，而它在货架上的位置这个特征不影响最终决策。

实际很多数据不会有这么直观的、根据生活经验即可判断的特征，如何选择特征就成了一个关键问题。为了选出更好的特征，有多种不同的量化评估方法，从而衍生出不同的决策树，最常用的有`ID3`(信息增益)、`C4.5`(信息增益比)和`CART`(Gini指数)这几个方法。

这里重点介绍`信息增益`。所谓信息增益，是指**划分数据集之后信息产生的变化量**，信息增益越高，说明该特征越重要，反之亦然。如何计算信息增益，则要从香农熵开始说起。



##### 香农熵

集合信息的度量方式称为香农熵或者简称为熵(entropy)。一条信息的信息量大小和它的不确定性有直接的关系，比如要搞清楚一件非常不确定的事乃至一无所知的事情，就需要了解大量的信息。而如果对某件事已经有了较多的了解，则不需要太多的信息就能把它搞清楚，所以信息量的度量与不确定性的程度成正比。

在信息论中，事件会有`n`种分类或者说`n`种可能的子事件`(x1 ~ xn)`，其中分类成`xi子事件`的信息定义为：

![dectree2.png](https://s.im5i.com/2021/01/12/dectree2.png)

其中`p(xi)`是该子事件发生的概率，式中的对数以2为底，也可以以e为底。可能很多人和我一样至今不知道为啥这样就是信息了，别问，问就是不知道ヽ(`Д´)ﾉ ┻━┻

上式可以计算出一个事件的信息，而计算香农熵则需要计算事件的所有可能子事件的信息期望值，公式如下：

![dectree3.png](https://s.im5i.com/2021/01/12/dectree3.png)

式中`n`是分类的数目。熵越大则表示随机变量的不确定性越大。



##### 经验熵

当熵中的概率是由数据估计(例如最大似然估计)得到时，对应的熵称为`经验熵(empirical entropy)`。

所谓数据估计也很简单：例如一共10组数据，分为A类和B类两个类别，其中A类数据有6组，B类数据有4组，则A类的概率为十分之六，B类的概率为十分之四。一般的，对于一个可以分成 `K`个类别的数据集`D`，定义其经验熵为`H(D)`，则经验熵公式为：

![dectree4.png](https://s.im5i.com/2021/01/13/dectree4.png)

其中`|D|`表示其样本容量，C<sub>k</sub>为属于第`k`类样本的数目。

其实这还是上述香农熵的公式，只是换了个说法。那么可以据此公式计算上述这个共有10组数据的简单数据集的经验熵：

H(D) = -(6/10)log<sub>2</sub>(6/10) - (4/10)log<sub>2</sub>(4/10)  = 0.971



##### 经验熵代码

创建一个容量为10的数据集，其中前4列为每组数据的四项特征的量化值，第5列为每组数据的标签，这里标签`N`设置了6组，标签`Y`设置了4组。`ShannonEntropy`方法是计算香农熵的方法。

```python
from math import log

def create_dataset():
    dataset = [[0, 0, 0, 0, 'N'],
               [0, 0, 0, 1, 'N'],
               [0, 1, 0, 1, 'Y'],
               [1, 0, 0, 0, 'N'],
               [1, 0, 0, 1, 'N'],
               [1, 1, 1, 1, 'Y'],
               [2, 0, 1, 2, 'Y'],
               [2, 0, 1, 0, 'N'],
               [2, 1, 0, 1, 'Y'],
               [2, 0, 0, 0, 'N']]
    labels = ['A', 'B', 'C', 'D']
    return dataset, labels

def ShannonEntropy(dataset):
    label_count = {}
    for data in dataset:
        current_label = data[-1]
        if current_label not in label_count.keys():
            label_count[current_label] = 0
        label_count[current_label] += 1

    result = 0.0
    for key in label_count:
        prob = float(label_count[key]) / len(dataset)
        result -= prob * log(prob, 2)
    return result

if __name__ == '__main__':
    dataset, _ = create_dataset()
    print(ShannonEntropy(dataset))
-----------------------------------------------------------------------
> 0.9709505944546686
```

##### 条件熵

知道如何计算经验熵之后是不是可以计算信息增益了呢？不行的，还差个`条件熵`。`条件熵H(Y|X)`表示在已知随机变量`X`的条件下随机变量`Y`的不确定性。`H(X|Y)` 定义为在给定条件 X 下，Y 的条件概率分布的熵对 X 的数学期望：

![dectree5.jpg](https://s.im5i.com/2021/01/13/dectree5.jpg)

其中                                  ![dectree6.jpg](https://s.im5i.com/2021/01/13/dectree6.jpg)

当条件熵中的概率是由数据估计(例如极大似然估计)得到时，所对应的条件熵称为`条件经验熵`。



##### 信息增益

至此便可以计算信息增益的量了，信息增益其实就是不同特征对数据集分类能力的量化。在信息论中，定义`互信息`为熵与条件熵的差值，而决策树学习中某个特征的信息增益就是数据集中标签类与该特征的互信息。

例如记特征A 对数据集D 的信息增益为`g(D, A)`，那么数据集D 的经验熵H(D)与特征A 给定条件下D的经验条件熵`H(D|A)`之差即为信息增益g(D, A)：

![dectree7.jpg](https://s.im5i.com/2021/01/13/dectree7.jpg)

设数据集`D`容量为`M`，特征`A` 有`n(n<=M)`个不同的取值`{a1, a2, ..., an}`，那么根据特征A 的不同取值可以将数据集划分为 n 个子集`{D1, D2, ..., Dn}`，其中第`i`个子集`Di`的样本容量为`|Di|`。数据集有`k(k<=M)`种不同的标签，那么根据标签的取值可以将数据集划分为 k 个子集`{C1, C2, ...Ck}`，其中第`i`个子集`Ci`的样本容量为`|Ci|`。

记子集`Di`中属于`Ck`的样本集为`Dik`，即二者交集。则特征A 的经验条件熵公式为：

![dectree8.jpg](https://s.im5i.com/2021/01/13/dectree8.jpg)



仍然以上述 [代码中的数据集](https://vixeruntr.github.io/2021/01/12/ML%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#%E7%BB%8F%E9%AA%8C%E7%86%B5%E4%BB%A3%E7%A0%81) 为例：取第一列的特征值记作特征A，可以看到特征A里有三种不同的数值，分别有`3个0、3个1和4个2`，出现的概率依次为`3/10、3/10 和 4/10`。对于A特征值为`0`的三组数据，标签为N的概率为2/3，标签为Y的概率为1/3；对于A特征值为`1`的三组数据，标签为N的概率为2/3，标签为Y的概率为1/3；对于A特征值为`2`的三组数据，标签为N的概率为2/4，标签为Y的概率为2/4。这也就是上述概念中的`Dik`。那么特征A的信息增益计算如下：

```python
g(D, A) = H(D) - H(D|A)
        = H(D) - [0.3H(D1) + 0.3H(D2) + 0.4H(D3)]
        = 0.971 - [ 0.3[-2/3log(2/3) - 1/3log(1/3)] + 
                    0.3[-2/3log(2/3) - 1/3log(1/3)] + 
                    0.4[-1/2log(1/2) - 1/2log(1/2)]]
        = 0.971 - (0.3 x 0.918 + 0.3 x 0.918 + 0.4 x 1.0)	
        = 0.02
```



其实计算`H(D|A)`这一步就是根据特征A的不同取值数目(这里是3)，把数据集分成3个子集，再分别计算每个子集的信息熵，然后计算这3个信息熵的加权和，而权重就是这些子集容量与完整数据集容量的比值。后面三项特征的信息增益计算方式也是一样的，不再赘述。



##### 信息增益代码

给定一个数据集，以下代码可以计算出数据集每个特征的信息增益。

```python
def split_dataset(dataset, axis, value):
    son_dataset = []
    for data in dataset:
        if data[axis] == value:
            son_dataset.append(data)
    return son_dataset

def findBestFeature(dataset):
    InfoEntropy = ShannonEntropy(dataset)
    FeatureNum = len(dataset[0]) - 1
    bestInfoGain = 0.0
    bestFeatureIndex = -1
    # 按列顺序遍历不同的特征

    for feature_index in range(FeatureNum):
        current_feature_values = [data[feature_index] for data in dataset]
        unique_value = set(current_feature_values)
        # 条件熵

        ConditionalEntropy = 0.0
        for value in unique_value:
            # 根据某特征的不同取值划分数据集,分别计算信息熵
            
            son_dataset = split_dataset(dataset, feature_index, value)
            prob = len(son_dataset) / float(len(dataset))
            ConditionalEntropy += prob * ShannonEntropy(son_dataset)

        InfoGain = InfoEntropy - ConditionalEntropy
        print("第%d个特征的信息增益为 %.3f" % (feature_index, InfoGain))
        if InfoGain > bestInfoGain:
            bestInfoGain = InfoGain
            bestFeatureIndex = feature_index
    return bestFeatureIndex

if __name__ == '__main__':
    dataset, _ = create_dataset()
    print(findBestFeature(dataset))
-----------------------------------------------------------------------
> 第0个特征的信息增益为 0.020
> 第1个特征的信息增益为 0.557
> 第2个特征的信息增益为 0.091
> 第3个特征的信息增益为 0.485
> 1
```

<br/>

## 构建决策树

上面提到构建决策树的经典算法有`ID3`(信息增益)、`C4.5`(信息增益比)和`CART`(Gini指数)这几个，并且介绍了信息增益的概念和计算方法，所以这里介绍与之对应的`ID3算法`，同时也是最常用的。

##### ID3算法

ID3算法的核心是根据信息增益量递归构建决策树。即从根节点开始，对节点计算所有可能特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点，之后再对子节点**递归**地调用以上方法，直到所有特征的信息增益都很小或没有特征可以选择为止。

上述例子中，由于第1个特征的信息增益最大，所以选择特征1作为根节点的特征。由于特征1 的取值只有两种，所以在根节点处将数据集分成两个子集，此时每个子集都有3组特征(原先的特征1的数据不再需要)。此后，对这两个子集再分别计算信息增益，以此选取节点特征，递归进行直到结束。

##### 代码实现

递归创建决策树时有两个终止条件：

+ 当剩余的所有的类标签完全相同时，直接返回该类标签；
+ 当使用完了所有特征后，仍然不能将数据划分为仅包含唯一类别的子集时，则决策树构建失败。说明此时数据维度不够，这里挑选剩余标签中出现数量最多的那一个作为返回值。

```python
def majority_label(label_list):
    labelCount = {}
    for label in label_list:
        if label not in labelCount.keys():
            labelCount[label] = 0
        labelCount[label] += 1
    labelCount = sorted(labelCount.items(),
                        key=lambda x: x[1], reverse=True)
    return labelCount[0][0]

def createDecisionTree(dataset, labels, UsefulFeatures):
    label_list = [data[-1] for data in dataset]
    # 如果仅有一种label则直接返回该label
    
    if label_list.count(label_list[0]) == len(label_list):
        return label_list[0]
    # 遍历完所有特征后返回出现次数最多的类标签
    
    if len(dataset[0]) == 1 or len(labels) == 0:
        return majority_label(label_list)

    bestFeatureIndex = findBestFeature(dataset)
    bestFeatureLabel = labels[bestFeatureIndex]
    UsefulFeatures.append(bestFeatureLabel)
    DecisionTree = {bestFeatureLabel: {}}
    # 删除原列表中已经使用过的label
    
    del (labels[bestFeatureIndex])
    feature_values = [data[bestFeatureIndex] for data in dataset]
    unique_value = set(feature_values)
    for value in unique_value:
        labels_copy = labels.copy()
        DecisionTree[bestFeatureLabel][value] = createDecisionTree(
            split_dataset(dataset, bestFeatureIndex, value), 
            labels_copy, UsefulFeatures)
    return DecisionTree

if __name__ == '__main__':
    dataset, labels = create_dataset()
    UsefulFeatures = []
    DecisionTree = createDecisionTree(dataSet, labels, UsefulFeatures)
    print(DecisionTree)
-------------------------------------------------------------------------
> {'B': {0: {'D': {0: 'N', 1: 'N', 2: 'Y'}}, 1: 'Y'}}
```

这里返回一个表示决策树逻辑的字典，根据上面代码的输出可以看到：根节点为特征`B`，如果该特征的为`1`则直接返回`Y`，否则来到节点`D`继续判断。

##### 数据测试

当构建好一棵决策树后，就可以测试其它有标签的新数据进行准确率统计，或者给定其它无标签的新数据，计算分类结果。代码如下：

```python
def predict(DecisionTree, UsefulFeatures, test_data):
    result_label = None
    root_node = next(iter(DecisionTree))
    sonDecisionTree = DecisionTree[root_node]
    node_depth = UsefulFeatures.index(root_node)
    for node in sonDecisionTree.keys():
        if test_data[node_depth] == node:
            if type(sonDecisionTree[node]).__name__ == 'dict':
                result_label = predict(sonDecisionTree[node], 
                                       UsefulFeatures, test_data)
            else:
                result_label = sonDecisionTree[node]
    return result_label

if __name__ == '__main__':
    dataSet, labels = create_dataset()
    UsefulFeatures = []
    DecisionTree = createDecisionTree(dataSet, labels, UsefulFeatures)
    new_data = [0, 1, 1, 1]
    result = predict(DecisionTree, UsefulFeatures, new_data)
    print(result)
-------------------------------------------------------------------------
> N
```

可以看到给该决策树输入一组新数据`[0, 1, 1, 1]`，得到的分类结果为`N`。