---
layout:     post   				    
title:      决策树算法 				
subtitle:    
date:       2021-01-12 				
author:     WarmStar 						
header-img: img/blog13.jpg 	
catalog: true 				
tags:							
    - ML
---

## 简述

**决策树(decision tree)**是一种基本的分类与回归方法，基于树结构进行决策的，可以看作`if-then`规则的集合。一棵决策树包含**一个根节点**、**若干内部节点**和**若干叶子节点**。其中根节点包含所有样本点，内部节点作为决策节点(属性测试)，叶子节点对应决策结果。

##### 决策原理

先由决策树的`根节点(root node)`到`叶子节点(leaf node)`的每一条路径构建一条规则，路径上决策节点的特征对应着规则的条件，叶子节点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：***互斥且完备***，即每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。实际用决策树进行分类是从根节点开始，对实例的某一特征进行测试，根据测试结果将实例分配到其子节点，若该子节点仍为决策节点，则继续进行判断与分配，直至将实例分到叶子节点的类中。

##### 重要概念

+ 根节点(Root Node)：表示整个样本集合，并且该节点可以进一步划分成两个或多个子集。
+ 决策节点(Decision Node)：当一个子节点可以被进一步拆分成多个子节点时，这个子节点就叫做决策节点。
+ 叶子节点(Leaf/Terminal Node)：无法再拆分的节点被称为叶子节点。
+ 拆分(Splitting)：表示将一个节点拆分成多个子集的过程。

+ 剪枝(Pruning)：移除决策树中无用子节点的过程就叫做剪枝，与拆分过程相反。

+ 分支/子树(Branch/Sub-Tree)：决策树的某一部分叫做分支或子树。

##### 实例

看下面一个决策树实例，数据来自 [ProcessOn公开数据](https://www.processon.com/view/57368ab8e4b0d4e09772f2a5?fromnew=1)

![dectree1.jpg](https://s.im5i.com/2021/01/12/dectree1.jpg)

这是一个根据天气状况决定是否出行的决策过程：如果是阴天直接选择出行，如果是晴天再根据空气湿度判断是否出行，如果是雨天再根据是否有风判断是否出行。其实生活中决策树算法的思想很常用，例如买衣服时，看到一款衣服决定要不要买要经过价格、品牌、款式和舒适度等条件的判断，脑海中纠结的过程其实就是一棵决策树。

<br/>

## 构建决策树

##### 算法流程

- 收集数据：使用任何方法收集整理所需数据。
- 准备数据：将收集的数据按照一定规则整理和存储。
- 分析数据：对数据进行特征筛选，数据变换等操作，目的是为了更好的训练。
- 训练算法：即构造决策树，也可以叫做决策树学习。
- 测试算法：使用训练好的决策树计算准确率。
- 优化算法：根据测得的准确率对数据或算法进行优化。

##### 特征选择

特征选择是为了选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。例如挑选衣服时，主要关注的是价格、舒适度等特征，而它在货架上的位置这个特征不影响最终决策。特征选择有不同的量化评估方法，从而衍生出不同的决策树，最常用的有`ID3`(信息增益)、`C4.5`(信息增益比)和`CART`(Gini指数)这几个方法。