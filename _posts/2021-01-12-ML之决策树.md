---
layout:     post   				    
title:      决策树算法 				
subtitle:    
date:       2021-01-12 				
author:     WarmStar 						
header-img: img/blog13.jpg 	
catalog: true 				
tags:							
    - ML
---

> 本文介绍的有关“熵”的概念，如需详细了解请阅读《信息论与编码》等书。
>
> 概率和条件概率等概念，如需详细了解请阅读《概率论与数理统计》等书。



## 简述

**决策树(decision tree)**是一种基本的分类与回归方法，基于树结构进行决策的，可以看作`if-then`规则的集合。一棵决策树包含**一个根节点**、**若干内部节点**和**若干叶子节点**。其中根节点包含所有样本点，内部节点作为决策节点(属性测试)，叶子节点对应决策结果。



##### 重要概念

+ 根节点(Root Node)：表示整个样本集合，并且该节点可以进一步划分成两个或多个子集。
+ 决策节点(Decision Node)：当一个子节点可以被进一步拆分成多个子节点时，这个子节点就叫做决策节点。
+ 叶子节点(Leaf/Terminal Node)：无法再拆分的节点被称为叶子节点。
+ 拆分(Splitting)：表示将一个节点拆分成多个子集的过程。
+ 剪枝(Pruning)：移除决策树中无用子节点的过程就叫做剪枝，与拆分过程相反。
+ 分支/子树(Branch/Sub-Tree)：决策树的某一部分叫做分支或子树。



##### 决策原理

先由决策树的`根节点(root node)`到`叶子节点(leaf node)`的每一条路径构建一条规则，路径上决策节点的特征对应着规则的条件，叶子节点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：***互斥且完备***，即每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。实际用决策树进行分类是从根节点开始，对实例的某一特征进行测试，根据测试结果将实例分配到其子节点，若该子节点仍为决策节点，则继续进行判断与分配，直至将实例分到叶子节点的类中。



##### 实例

看下面一个决策树实例，数据来自 [ProcessOn公开数据](https://www.processon.com/view/57368ab8e4b0d4e09772f2a5?fromnew=1)

![dectree1.jpg](https://s.im5i.com/2021/01/12/dectree1.jpg)

这是一个根据天气状况决定是否出行的决策过程：如果是阴天直接选择出行，如果是晴天再根据空气湿度判断是否出行，如果是雨天再根据是否有风判断是否出行。其实生活中决策树算法的思想很常用，例如买衣服时，看到一款衣服决定是否购买要经过价格、品牌、款式和舒适度等条件的判断对比，脑海中纠结的过程其实就是一棵决策树。



##### 算法流程

构建一个决策树算法的主要步骤大致如下：

- 收集数据：使用任何方法收集整理所需数据。
- 准备数据：将收集的数据按照一定规则整理和存储。
- 分析数据：对数据进行特征筛选，数据变换、清洗等操作。
- 训练算法：即构造决策树，也可以叫做决策树学习。
- 测试算法：使用训练好的决策树计算准确率。
- 优化算法：根据测得的准确率对数据或算法进行优化。

<br/>

## 信息增益

假设现在已经有了一份整理好的数据集，那么在训练决策树之前，需要对数据集进行分析、清洗变换和特征选择等操作，其中特征选择是最重要的一环。

##### 特征选择

特征选择是为了选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。例如挑选衣服时，主要关注的是价格、舒适度等特征，而它在货架上的位置这个特征不影响最终决策。

实际很多数据不会有这么直观的、根据生活经验即可判断的特征，如何选择特征就成了一个关键问题。为了选出更好的特征，有多种不同的量化评估方法，从而衍生出不同的决策树，最常用的有`ID3`(信息增益)、`C4.5`(信息增益比)和`CART`(Gini指数)这几个方法。

这里重点介绍`信息增益`。所谓信息增益，是指**划分数据集之后信息产生的变化量**，信息增益越高，说明该特征越重要，反之亦然。如何计算信息增益，则要从香农熵开始说起。



##### 香农熵

集合信息的度量方式称为香农熵或者简称为熵(entropy)。一条信息的信息量大小和它的不确定性有直接的关系，比如要搞清楚一件非常不确定的事乃至一无所知的事情，就需要了解大量的信息。而如果对某件事已经有了较多的了解，则不需要太多的信息就能把它搞清楚，所以信息量的度量与不确定性的程度成正比。

在信息论中，事件会有`n`种分类或者说`n`种可能的子事件`(x1 ~ xn)`，其中分类成`xi子事件`的信息定义为：

![dectree2.png](https://s.im5i.com/2021/01/12/dectree2.png)

其中`p(xi)`是该子事件发生的概率，式中的对数以2为底，也可以以e为底。可能很多人和我一样至今不知道为啥这样就是信息了，别问，问就是不知道ヽ(`Д´)ﾉ ┻━┻

上式可以计算出一个事件的信息，而计算香农熵则需要计算事件的所有可能子事件的信息期望值，公式如下：

![dectree3.png](https://s.im5i.com/2021/01/12/dectree3.png)

式中`n`是分类的数目。熵越大则表示随机变量的不确定性越大。



##### 经验熵

当熵中的概率是由数据估计(例如最大似然估计)得到时，对应的熵称为`经验熵(empirical entropy)`。

所谓数据估计也很简单：例如一共10组数据，分为A类和B类两个类别，其中A类数据有6组，B类数据有4组，则A类的概率为十分之六，B类的概率为十分之四。一般的，对于一个可以分成 `K`个类别的数据集`D`，定义其经验熵为`H(D)`，则经验熵公式为：

![dectree4.png](https://s.im5i.com/2021/01/13/dectree4.png)

其中`|D|`表示其样本容量，C<sub>k</sub>为属于第`k`类样本的数目。

其实这还是上述香农熵的公式，只是换了个说法。那么可以据此公式计算上述这个共有10组数据的简单数据集的经验熵：

H(D) = -(6/10)log<sub>2</sub>(6/10) - (4/10)log<sub>2</sub>(4/10)  = 0.971



##### 经验熵Python代码

创建一个容量为10的数据集，其中前4列为每组数据的四项特征的量化值，第5列为每组数据的标签，这里标签`N`设置了6组，标签`Y`设置了4组。`ShannonEntropy`方法是计算香农熵的方法。

```python
from math import log

def create_dataset():
    dataset = [[0, 0, 0, 0, 'N'],
               [0, 0, 0, 1, 'N'],
               [0, 1, 0, 1, 'Y'],
               [1, 0, 0, 0, 'N'],
               [1, 0, 0, 1, 'N'],
               [1, 1, 1, 1, 'Y'],
               [2, 0, 1, 2, 'Y'],
               [2, 0, 1, 0, 'N'],
               [2, 1, 0, 1, 'Y'],
               [2, 0, 0, 0, 'N']]
    return dataset

def ShannonEntropy(dataset):
    label_count = {}
    for data in dataset:
        current_label = data[-1]
        if current_label not in label_count.keys():
            label_count[current_label] = 0
        label_count[current_label] += 1

    result = 0.0
    for key in label_count:
        prob = float(label_count[key]) / len(dataset)
        result -= prob * log(prob, 2)
    return result

if __name__ == '__main__':
    dataset = create_dataset()
    print(ShannonEntropy(dataset))
-----------------------------------------------------------------------
> 0.9709505944546686
```

##### 信息增益

知道如何计算经验熵之后是不是可以计算信息增益了呢？不行的，还差个`条件熵`。`条件熵H(Y|X)`表示在已知随机变量`X`的条件下随机变量`Y`的不确定性。H(X|Y) 定义为在给定条件 X 下，Y 的条件概率分布的熵对 X 的数学期望：

![dectree5.jpg](https://s.im5i.com/2021/01/13/dectree5.jpg)

其中                                  ![dectree6.jpg](https://s.im5i.com/2021/01/13/dectree6.jpg)

当条件熵中的概率是由数据估计(例如极大似然估计)得到时，所对应的条件熵称为条件经验熵。