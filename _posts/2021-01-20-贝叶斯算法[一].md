---
layout:     post   				    
title:      朴素贝叶斯算法[一] 				
subtitle:   
date:       2021-01-20 				
author:     WarmStar 						
header-img: img/blog15.jpg 	
catalog: true 				
tags:							
    - ML
---

## 贝叶斯决策论

贝叶斯决策论是以贝叶斯原理为基础，使用概率统计的知识对样本数据集进行分类。贝叶斯算法的特点是结合`先验概率`和`后验概率`，即避免了只使用先验概率的主观偏见，也避免了单独使用样本信息而出现的过拟合现象，在数据集较大的情况下表现出较高的准确率。由于该方法要求所有相关概率都已知，而实际场景中很难获得完备的概率数据，所以应用条件非常苛刻。

为了理解贝叶斯算法，需要先理解`条件概率`的概念和计算方法。

##### 条件概率

设A, B是两个事件，且P(B) > 0，将事件B发生的条件下，事件A发生的概率记为`P(A|B)`。

![kDitw.jpg](https://s.im5i.com/2021/01/28/kDitw.jpg)



由文氏图可以看到，事件A和B同时发生的概率为`P(A∩B)`，那么B发生的条件下A 发生的概率为

`P(A|B) = P(A∩B)/P(B)`

由上式移项得  `P(A∩B) = P(A|B)P(B)`

同理可以得到  `P(A∩B) = P(B|A)P(A)`

联立两式得到   `P(A|B)P(B) = P(B|A)P(A)`

移项可得条件概率公式：![kDd2Y.jpg](https://s.im5i.com/2021/01/28/kDd2Y.jpg)



##### 全概率公式

上述条件概率公式中的`P(B)`的值，实际场景中可以使用全概率公式计算得到，即将一个复杂事件的概率求解问题转化为在不同情况下发生的简单事件的概率求和问题。

![kDYVd.jpg](https://s.im5i.com/2021/01/28/kDYVd.jpg)

如图，假设`S`为样本空间，它是事件`A`与`A'`的和集。此时事件`B`可以表示为：

`P(B) = P(B∩A) + P(B∩A')`

由于已知  `P(A∩B) = P(B|A)P(A)`

代入可得  `P(B) = P(B|A)P(A) + P(B|A')P(A')`。此式就是全概率公式。

将全概率公式代入到条件概率公式可得：

![kDgOK.jpg](https://s.im5i.com/2021/01/28/kDgOK.jpg)

一般地，如果事件`B1, B2, B3, …, Bn`构成一个完备事件组`B`，子事件之间互不相容，并且`P(Bi) > 0`，则对任一事件`A`有：`P(A) = P(A|B1)P(B1) + P(A|B2)P(B2) + ... + P(A|Bn)P(Bn)`



##### 贝叶斯推断

回到条件概率公式上来，将条件概率公式变形得到：![kDLYF.jpg](https://s.im5i.com/2021/01/28/kDLYF.jpg)

+ 其中`P(A)`称为`先验概率`，即事件B发生**之前**对事件A的概率的判断。

+ `P(A|B)`称为`后验概率`，即事件B发生**之后**对事件A的概率的重新评估。

+ `P(B|A)/P(B)`称为`可能性函数`，这是一个使得预估概率更接近真实概率的`调整因子`。
  + 如果调整因子的值大于1，意味着先验概率被增强，事件A的发生的可能性变大；
  + 如果调整因子的值等于1，意味着事件B的发生不影响事件A的发生概率；
  + 如果调整因子的值小于1，意味着先验概率被减弱，事件A的发生的可能性变小。

所以贝叶斯推断可描述为`后验概率 = 先验概率 x 调整因子`，即先预估一个先验概率，然后加入实验结果(可能性函数)，看这个实验对先验概率产生了怎样的影响，由此得到更接近实际的后验概率。



##### 实例

其实条件概率举个最直白的例子就是：现在知道一个生物是个人(已知条件)，那这个生物是男人的概率是50%；现在知道另一个生物是个男人(已知条件)，那这个生物是人的概率是100%。由于已知条件的不同，得到和它有关的事件的概率也会随之改变。

为了更好地理解条件概率和全概率公式，再来一道比较简单的《概率论与数理统计》里面的题目：现有两个同样的口袋，一号口袋有黑球30颗和白球10颗，二号口袋有黑球白球各20颗，现在任选一个口袋取出一颗球，发现是黑球，求这颗黑球来自一号口袋的概率。

解答：定义选择一号口袋的概率为 `P(G1)`，选择二号口袋的概率为 `P(G2)`，显然 `P(G1)=P(G2)=0.5`，这也就是先验概率，即在没有取球之前，来自两个口袋的概率都是0.5。定义取出黑球的概率为 `P(B)`，则问题就转化了计算`P(G1|B)`的值。

根据条件概率公式可知 `P(G1|B) = P(G1)P(B|G1)/P(B)`

已知 `P(G1)=0.5`，`P(B|G1)` 为从一号口袋取出黑球的概率，显然`P(B|G1)=30/(30+10)=0.75`

`P(B)`是取出黑球的概率，可以利用全概率公式计算：

 `P(B) = P(B|G1)P(G1) + P(B|G2)|G2 = 0.75*0.5 + 0.5*0.5 = 0.625`

最终可得 `P(G1|B) = 0.5*0.75/0.625 = 0.6`

根据计算过程中`0.75/0.625 = 1.2 > 1`以及前述有关`调整因子`的内容可知：来自一号口袋的先验概率为0.5，但是一旦发现取出的球是黑球后，它来自一号口袋的可能性被增强了(实际就是因为一号口袋黑球多)。计算`P(G2|B)`也是同样的道理。另外观察`P(G1|B) = P(G1)P(B|G1)/P(B)`和`P(G2|B) = P(G2)P(B|G2)/P(B)`两个式子可以发现，式中的分母都是`P(B)`，所以实际用贝叶斯方法进行分类任务时，无需计算`P(B)`，只需找出分子最大的那个即可。

<br/>

## 朴素贝叶斯

朴素贝叶斯算法是基于**贝叶斯决策论**与**特征条件独立性假设**的分类方法。所谓特征条件独立性假设，即对于已知的类别，所有特征相互独立，换句话说就是每一种特征独立地对分类结果产生影响。这一特点其实也就是`朴素`的含义。

##### 特征条件独立

朴素贝叶斯算法和贝叶斯算法的区别就在于朴素贝叶斯基于**特征条件独立性假设**，反映到公式上：已知事件B有`n`个独立特征`{B1, B2, …, Bn}`，那么在事件B发生的条件下事件A发生的条件概率为  `P(A|B) = P(A)P(B|A)/P(B) = P(A)P(B1,B2,…,Bn|A)/P(B1,B2,…,Bn)`

由于每个特征都是独立地对结果产生影响，所以可以进一步得到：

 `P(A|B) = P(A)P(B1|A)P(B2|A)P(B3|A)…P(Bn|A)/P(B1)P(B2)P(B3)…P(Bn)`

而没有**特征条件独立**这一前提条件(贝叶斯算法)时，此式是不成立的。



##### 实例

某家企业收到了8份简历，他们的学历情况和最终录用情况(仅考虑学历)如下表所示：

| 最高学历 | 学校排名 | 录用结果 |
| :------: | :------: | :------: |
|   硕士   |   985    |   录用   |
|   硕士   |   211    |   录用   |
|   硕士   |   普本   |  未录用  |
|   硕士   |   211    |   录用   |
|   本科   |   211    |  未录用  |
|   本科   |   985    |   录用   |
|   本科   |   普本   |   录用   |
|   本科   |   普本   |  未录用  |

然后又收到了一份简历，他的学历情况是：硕士、普本院校，求他被录用的概率。

解答：定义录用事件为`P(A)`，学历情况为`P(B)`，最高学历为`P(B1)`，学校排名为`P(B2)`，则问题转化为计算  `P(A|B) = P(A)P(B|A)/P(B) = P(A)P(B1|A)P(B2|A)/P(B1)P(B2)`

为了方便表述这里转化为文字： `P(录用|硕士、普本) = P(录用)P(硕士|录用)P(普本|录用)/P(硕士)P(普本) = (5/8 * 3/5 * 1/5)/(1/2 * 3/8) = 0.4` 

上面计算时用到的概率值就是查数据集得到的，例如`P(硕士|录用) = 3/5`的来源就是：录用结果里显示为`录用`的共有5个，这5个录用的人里最高学历为`硕士`的有3个。其它概率值同理。

同样也可以计算不录用的概率：  `P(未录用|硕士、普本) = P(未录用)P(硕士|未录用)P(普本|未录用)/P(硕士)P(普本) = (3/8 * 1/3 * 2/3)/(1/2 * 3/8) = 4/9 `

<br/>

## 代码实现

##### 数据集

实现一个辨别语句中是否包含具有攻击性的词汇的功能(类似敏感词屏蔽)，可以使用朴素贝叶斯对现有语料数据进行建模分类，找出一些大概率具有攻击性的词汇加入数据库标记。首先是数据集(数据来自网络)：

+ 在`create_dataset`函数中创建了几个句子，已切分成单词存储，`labels`列表的值表示`words_list`列表中对应索引的句子是否具有攻击性，0表示无攻击性，1表示有攻击性。
+ `create_glossary`函数用以创建一个词汇表，保存`words_list`中出现的**所有不重复单词**。
+ `words2vector`函数用以根据词汇表将`words_list`的每个句子向量化：对于每个句子都创建一个和词汇表等长度的空列表，对于该句子中的每个单词，如果单词存在于词汇表中，则将空列表中该单词在词汇表中索引处的值置1，其余位置都置0。

```python
def create_dataset():
    words_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                  ['my', 'dog', 'is', 'so', 'cute', 'I', 'love', 'him'],
                  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    labels = [0, 1, 0, 1, 0, 1]
    return words_list, labels

def create_glossary(words_list):
    glossary = set([])
    for words in words_list:
        glossary = glossary | set(words)
    return list(glossary)

def words2vector(glossary, words):
    words_matrix = [0] * len(glossary)
    for word in words:
        if word in glossary:
            words_matrix[glossary.index(word)] = 1
        else:
            print("The word: %s not in Glossary!" % word)
    return words_matrix

if __name__ == '__main__':
    words_list, labels = create_dataset()
    glossary = create_glossary(words_list)
    words_matrix = []
    for words in words_list:
        words_matrix.append(words2vector(glossary, words))
```

##### 训练

```python
import numpy as np

def NaiveBayesian(words_matrix, labels):
    words_num = len(words_matrix)
    glossary_len = len(words_matrix[0])
    p1_prob = sum(labels)/float(words_num)
    p0_word = np.zeros(glossary_len)
    p1_word = np.zeros(glossary_len)
    p0_num = 0.0
    p1_num = 0.0
    for i in range(words_num):
        if labels[i] == 0:
            p0_word += words_matrix[i]
            p0_num += sum(words_matrix[i])
        else:
            p1_word += words_matrix[i]
            p1_num += sum(words_matrix[i])
    p0_vector = p0_word / p0_num
    p1_vector = p1_word / p1_num
    return p0_vector, p1_vector, p1_prob

if __name__ == '__main__':
    words_list, labels = create_dataset()
    glossary = create_glossary(words_list)
    words_matrix = []
    for words in words_list:
        words_matrix.append(words2vector(glossary, words))
    print(glossary)
    p0_vector, p1_vector, p1_prob = NaiveBayesian(words_matrix, labels)
    print(p0_vector)
    print(p1_vector)
    print(p1_prob)
```

打印结果(部分)如下图：

![kDnDC.jpg](https://s.im5i.com/2021/01/28/kDnDC.jpg)

打印的第一个列表是词汇表，第二个列表是词汇表中每个单词**不具有攻击性**的概率，第三个列表是词汇表中每个单词**具有攻击性**的概率，`p1_prob`就是词汇具有攻击性的先验概率。例如`dog`这个单词，根据打印结果可知该单词不具有攻击性的概率为`0.083`，具有攻击性的概率为`0.105`；以及`cute`这个单词，根据打印结果可知该单词不具有攻击性的概率为`0.041`，具有攻击性的概率为`0.000`。

##### 预测

```python
from functools import reduce

def predict(words_vector, p0_vector, p1_vector, prior_prob):
    p1 = reduce(lambda x, y: x * y, words_vector * p1_vector) * prior_prob
    p0 = reduce(lambda x, y: x * y, words_vector * p0_vector) * (1.0 - prior_prob)
    print('无攻击性的概率:', p0)
    print('有攻击性的概率:', p1)

if __name__ == '__main__':
    words_list, labels = create_dataset()
    glossary = create_glossary(words_list)
    words_matrix = []
    for words in words_list:
        words_matrix.append(words2vector(glossary, words))
    p0_vector, p1_vector, p1_prob = NaiveBayesian(words_matrix, labels)

    words = ['I', 'love', 'my', 'cute', 'dog']
    words_vector = np.array(words2vector(glossary, words))
    predict(words_vector, p0_vector, p1_vector, p1_prob)
------------------------------------------------------------------------
> 无攻击性的概率: 0.0
> 有攻击性的概率: 0.0 
```

从打印结果可以看到，两个类别的概率都是0，这样是无法比较进而得出结论的，显然计算过程中出了问题。为了方便说明，还是看 [上面简历筛选的例子](https://vixeruntr.github.io/2021/01/20/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95-%E4%B8%80/#%E5%AE%9E%E4%BE%8B-1) ，假设现在然后又收到了一份简历，他的学历情况是：硕士、985院校，求他不被录用的概率。

同样的，套用公式得到`P(未录用|硕士、985) = P(未录用)P(硕士|未录用)P(985|未录用)/P(硕士)P(985) `，但是计算时发现`P(985|未录用)`的概率是0，原因是数据集中并没有985院校且未录用的案例，所以式中只要有一个值为0，那么最终结果就是0。这是数据集不够完备的原因造成的，为了解决这一问题，可以引入`拉普拉斯平滑`。



##### 拉普拉斯平滑

拉普拉斯平滑也叫加一平滑，专门解决0概率问题，它的操作非常简单，就是直接把每个类别里所有划分的计数**加1**，只要数据集足够大，对最终结果产生的影响就可以忽略。

为了方便表示，将先验概率计算公式表示为：![kDaIp.jpg](https://s.im5i.com/2021/01/28/kDaIp.jpg)

其中D<sub>c</sub> 表示`训练集D`中第`c`类的样本集，`P(c)`表示该类别的先验概率。

将条件概率公式表示为：![kDJa7.jpg](https://s.im5i.com/2021/01/28/kDJa7.jpg)

其中D<sub>c,xi</sub> 表示D<sub>c</sub> 在第`i`个属性上取值为 x<sub>i</sub> 的样本集。

则**引入拉普拉斯平滑后**，先验概率计算公式转化为：![kDyEv.jpg](https://s.im5i.com/2021/01/28/kDyEv.jpg)

其中`N`表示训练集D 中可能的类别数。

条件概率计算公式转化为：![kD67G.jpg](https://s.im5i.com/2021/01/28/kD67G.jpg)

其中N<sub>i</sub> 表示第`i`个属性可能的取值数。

<br/>

回到`P(未录用|硕士、985) = P(未录用)P(硕士|未录用)P(985|未录用)/P(硕士)P(985) `这个式子中来，引入拉普拉斯平滑算法后：

+ 先验概率`P(未录用)`就计算为`(3+1)/(8+2) = 2/5`。数据集总大小为8，`N=2 (录用和未录用2种类别)`；未录用的子集有3个。
+ `P(硕士|未录用)`的概率就是`(1+1)/(3+2) = 2/5`。此时未录用的数目为3，N<sub>i</sub>为2 (硕士和本科2种类别)；未录用的子集中硕士学历有1个。
+ `P(985|未录用)`的概率就是`(0+1)/(3+3) = 1/6`。此时未录用的数目为3，N<sub>i</sub>为3 (985、211和普本3种类别)；未录用的子集中985学校的有0个。

如果此时又进来一份简历，他的学历情况是：`博士`、985院校，求他不被录用的概率。那么此时就是计算 `P(未录用|博士、985) = P(未录用)P(博士|未录用)P(985|未录用)/P(博士)P(985) `，虽然数据集中没有一条最高学历是博士的数据，但依据拉普拉斯平滑依旧可以计算：

+ 先验概率`P(未录用)`就计算为`(3+1)/(8+2) = 2/5`。数据集总大小为8，`N=2 (录用和未录用2种类别)`；未录用的子集有3个。
+ `P(博士|未录用)`的概率就是`(0+1)/(3+3) = 1/6`。此时未录用的数目为3，N<sub>i</sub>为3 (博士、硕士和本科3种类别)；未录用的子集中博士学历有0个。
+ `P(985|未录用)`的概率就是`(0+1)/(3+3) = 1/6`。此时未录用的数目为3，N<sub>i</sub>为3 (985、211和普本3种类别)；未录用的子集中985学校的有0个。



##### 下溢出

解决了0概率问题后，还有一个可能出现的问题是`下溢出`，即多个小数连乘的情况下，程序的小数位四舍五入后，最终结果可能会变成0。解决方法也很简单，**对成绩结果取自然对数**，这样的转换操作可以避免下溢出问题和浮点数舍入导致的错误。

![kdT8z.jpg](https://s.im5i.com/2021/01/29/kdT8z.jpg)

观察上图的两个曲线：蓝色曲线`f(x)`是一条符合正弦函数变化规律的曲线，`x轴`取值范围`[0, 2π]`，`y轴`取值范围限制在`(0, 1)`；黄色曲线为`In(f(x))`。可以看到这两条曲线在相同区域内的值同时变大或变小，并且在同一点上取到极值。据此，可以在不影响最终结果的情况下消除下溢出问题。



##### 优化后的代码

这里只贴出了修改了的几行代码。

```python
def NaiveBayesian(words_matrix, labels):
    p0_word = np.ones(glossary_len)
    p1_word = np.ones(glossary_len)
    p0_num = 2.0
    p1_num = 2.0
    
def predict(words_vector, p0_vector, p1_vector, prior_prob):
    p1 = sum(words_vector * p1_vector) + np.log(prior_prob)
    p0 = sum(words_vector * p0_vector) + np.log(1 - prior_prob)
```





