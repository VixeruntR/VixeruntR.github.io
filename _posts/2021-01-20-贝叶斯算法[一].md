---
layout:     post   				    
title:      朴素贝叶斯算法[一] 				
subtitle:   
date:       2021-01-20 				
author:     WarmStar 						
header-img: img/blog15.jpg 	
catalog: true 				
tags:							
    - ML
---

## 贝叶斯决策论

贝叶斯决策论是以贝叶斯原理为基础，使用概率统计的知识对样本数据集进行分类。贝叶斯方法的特点是结合`先验概率`和`后验概率`，即避免了只使用先验概率的主观偏见，也避免了单独使用样本信息而出现的过拟合现象，在数据集较大的情况下表现出较高的准确率。由于该方法要求所有相关概率都已知，而实际场景中后验概率很难获得，所以应用条件非常苛刻。

为了理解贝叶斯方法，需要先理解`条件概率`的概念和计算方法。

##### 条件概率

设A, B是两个事件，且P(B) > 0，将事件B发生的条件下，事件A发生的概率记为`P(A|B)`。

![hsmyB.jpg](https://s.im5i.com/2021/01/22/hsmyB.jpg)



由文氏图可以看到，事件A和B同时发生的概率为`P(A∩B)`，那么B发生的条件下A 发生的概率为

`P(A|B) = P(A∩B)/P(B)`

由上式移项得  `P(A∩B) = P(A|B)P(B)`

同理可以得到  `P(A∩B) = P(B|A)P(A)`

联立两式得到   `P(A|B)P(B) = P(B|A)P(A)`

移项可得条件概率公式：![hnUSo.jpg](https://s.im5i.com/2021/01/22/hnUSo.jpg)



##### 全概率公式

上述条件概率公式中的`P(B)`的值，实际可以使用全概率公式计算得到，即将一个复杂事件的概率求解问题转化为在不同情况下发生的简单事件的概率求和问题。

![hMfjw.jpg](https://s.im5i.com/2021/01/22/hMfjw.jpg)

如图，假设`S`为样本空间，它是事件`A`与`A'`的和集。此时事件`B`可以表示为：

`P(B) = P(B∩A) + P(B∩A')`

由于已知  `P(A∩B) = P(B|A)P(A)`

代入可得  `P(B) = P(B|A)P(A) + P(B|A')P(A')`。此式就是全概率公式。

将全概率公式代入到条件概率公式可得：

![hMJSF.jpg](https://s.im5i.com/2021/01/22/hMJSF.jpg)

一般地，如果事件`B1, B2, B3, …, Bn`构成一个完备事件组`B`，子事件之间互不相容，并且`P(Bi) > 0`，则对任一事件`A`有：`P(A) = P(A|B1)P(B1) + P(A|B2)P(B2) + ... + P(A|Bn)P(Bn)`



##### 贝叶斯推断

将条件概率公式变形得到：![hnduW.jpg](https://s.im5i.com/2021/01/22/hnduW.jpg)

+ 其中`P(A)`称为`先验概率`，即事件B发生**之前**对事件A的概率的判断。

+ `P(B|A)`称为`后验概率`，即事件B发生**之后**对事件A的概率的重新评估。

+ `P(B|A)/P(B)`称为`可能性函数`，这是一个使得预估概率更接近真实概率的`调整因子`。
  + 如果调整因子的值大于1，意味着先验概率被增强，事件A的发生的可能性变大；
  + 如果调整因子的值等于1，意味着事件B的发生不影响事件A的发生概率；
  + 如果调整因子的值小于1，意味着先验概率被减弱，事件A的发生的可能性变小。

所以贝叶斯推断可描述为`后验概率 = 先验概率 x 调整因子`，即先预估一个先验概率，然后加入实验结果(可能性函数)，看这个实验对先验概率产生了怎样的影响，由此得到更接近实际的后验概率。



##### 实例

其实条件概率举个最直白的例子就是：现在知道一个生物是个人(已知条件)，那这个生物是男人的概率是50%；现在知道另一个生物是个男人(已知条件)，那这个生物是人的概率是100%。

为了更好地理解条件概率和全概率公式，再来一道比较简单的《概率论与数理统计》里面的题目：现有两个同样的口袋，一号口袋有黑球30颗和白球10颗，二号口袋有黑球白球各20颗，现在任选一个口袋取出一颗球，发现是黑球，求这颗黑球来自一号口袋的概率。

定义选择一号口袋的概率为 `P(G1)`，选择二号口袋的概率为 `P(G2)`，显然 `P(G1)=P(G2)=0.5`，这也就是先验概率，即在没有取球之前，来自两个口袋的概率都是0.5。定义取出黑球的概率为 `P(B)`，则问题就转化了计算`P(G1|B)`的值。

根据条件概率公式可知 `P(G1|B) = P(G1)P(B|G1)/P(B)`

已知 `P(G1)=0.5`，`P(B|G1)` 为从一号口袋取出黑球的概率，显然`P(B|G1)=30/(30+10)=0.75`

`P(B)`是取出黑球的概率，可以利用全概率公式计算：

 `P(B) = P(B|G1)P(G1) + P(B|G2)|G2 = 0.75*0.5 + 0.5*0.5 = 0.625`

最终可得 `P(G1|B) = 0.5*0.75/0.625 = 0.6`

根据计算过程重`0.75/0.625 = 1.2 > 1`以及上述有关`调整因子`的内容可知：来一一号口袋的先验概率为0.5，但是一旦发现取出球是黑球后，它来自一号口袋的可能性被增强了(实际就是因为一号口袋黑球多)。计算`P(G2|B)`也是同样的道理。另外观察`P(G1|B) = P(G1)P(B|G1)/P(B)`和`P(G2|B) = P(G2)P(B|G2)/P(B)`两个式子可以发现，式中的分母都是`P(B)`，所以实际用贝叶斯方法进行分类任务时，无需计算`P(B)`，只需找出分子最大的那个即可。

<br/>

## 朴素贝叶斯

朴素贝叶斯法是基于**贝叶斯决策论**与**特征条件独立性假设**的分类方法。所谓特征条件独立性假设，即对于已知的类别，所有特征相互独立，换句话说就是每一种特征独立地对分类结果产生影响。这一特点其实也就是`朴素`的含义。

反映到实际计算里就是：给定数据集后，对于一组没有分好类别的新数据，根据新数据的特征值，计算出该组数据属于数据集中每个类别的概率，得到的概率值最大的类即认为该组数据属于该类别。