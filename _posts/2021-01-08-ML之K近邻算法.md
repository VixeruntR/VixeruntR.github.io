---
layout:     post   				    
title:      K近邻算法 				
subtitle:    
date:       2021-01-08 				
author:     WarmStar 						
header-img: img/blog12.jpg 	
catalog: true 				
tags:							
    - ML
---

### 简述

K近邻算法(K-nearest neighbor, KNN)是一种**基本分类与回归方法**。

工作原理：有一个样本集，且样本集中的每一组数据都有一个标签，即已知每一组数据与其所属分类的对应关系。输入没有标签的新数据后，会将新数据的每个特征与样本集中的数据的对应特征相比较，然后提取样本集中最相似数据的分类标签。一般只选择样本集中`前K个`最相似的数据，这也就是K近邻算法中K的含义。最后选择这 K 个最相似数据中**出现次数最多**的标签作为新数据的类别。

##### 算法流程

+ 计算测试数据与各个训练数据之间的距离
+ 按照距离的递增关系进行排序
+ 选取距离最小的 K 个点
+ 确定前 K 个点所在类别的出现频率
+ 返回前 K 个点中出现频次最高的类别作为测试数据的预测分类

<br/>

### 简单实例

##### 简单样本集

使用 KNN算法分类一组简单的数据，根据玩过游戏种类的数量区分人物性别。

| 样本 | 换装 | 格斗 | 性别 |
| :--: | :--: | :--: | :--: |
|  A   |  2   |  40  |  男  |
|  B   |  4   |  35  |  男  |
|  C   |  47  |  3   |  女  |
|  D   |  49  |  1   |  女  |

上表就是一个简单的数据集，只有两个特征。根据一般人的认知经验来看，玩过换装游戏多的人大概率是女性，换过格斗游戏多的人大概率是男性。这时候如果再给出一组新数据：换装游戏玩过1款，格斗游戏玩过36款，人和KNN算法都可以给出这个人是男性的答案。但如果给出的数据是换装游戏玩过32款，格斗游戏玩过30款，人就不好判断了，但KNN不会纠结什么，它会根据**既定的规则**给出计算得到的结果，虽然这个结果未必与现实相符。

##### 距离度量

上面提到KNN会根据既定的规则来比较特征进行分类，这里就介绍一下这个既定的规则：**距离度量**，即将量化后的特征值代入距离计算公式计算距离。这里的例子中只有两种特征，那么用二维实数向量空间的坐标就可以表示了，距离计算公式为**对应坐标差的平方和再开根号**：

```python
|D| = [(x - x_)^2 + (y - y_)^2] ^ (0.5)
```

根据上文给出的**算法流程**计算新数据 (1, 36) 的类别，这里 K 取 3：

1. 分别计算样本集中各个数据的距离得到：4.12、3.16、56.61、59.4
2. 按增序排列后得到：3.16、4.12、56.61、59.4
3. 选取距离最近的 3 个点得到：3.16、4.12、56.61
4. 前3个点所属类别出现的频次为：男出现2次、女出现1次
5. 由于男出现频次最高，所以将(1, 36) 分类为男

以上就是一个简单KNN算法计算全过程。注意如果`K值取1`，这个算法就变成了`最近邻算法`，而非`K近邻算法`。

<br/>

### 手撕 KNN

为了加深理解KNN的原理和实现细节，下面用Python的基本库实现该算法。

##### 生成数据集

用`numpy`生成数据集矩阵，标签则用一个一维列表存储即可。数据形式参考如下输出。

```python
import numpy as np

def dataset():
    data = np.array([[2, 40], [4, 35], [47, 3], [49, 1]])
    label = ['男', '男', '女', '女']
    return data, label

if __name__ == '__main__':
    data, label = dataset_init()
    print(data)
    print(label)
-----------------------------------------------------------------------
>  [[ 2 40]
 	[ 4 35]
 	[47  3]
 	[49  1]]
>  ['男', '男', '女', '女']
```

##### KNN算法

使用 numpy 可以非常方便的将列表数据转化成矩阵，然后一次矩阵运算即可得到所有距离值。该算法代码实现非常简单，可参考如下示例。

```python
def dataset():
    data_ = np.array([[2, 40], [4, 35], [47, 3], [49, 1]])
    labels_ = ['男', '男', '女', '女']
    return data_, labels_

def knn(train_data, train_labels, test_data, k):
    # param train_data: 训练集(array type)
    
    # param train_labels: 训练集标签(list type)
    
    # param test_data: 需要分类的数据(list type)
    
    # param k: KNN算法的K值(int type)
    
    test_data = np.array(test_data)
    EuclideanDistance = (((test_data - train_data) ** 2).sum(axis=1)) ** 0.5
    sorted_index = EuclideanDistance.argsort()
    k_labels = [train_labels[i] for i in sorted_index[0: k]]
    result_label = collections.Counter(k_labels).most_common(1)
    return result_label[0][0]

if __name__ == '__main__':
    data, labels = dataset()
    result = knn(data, labels, [1, 36], 3)
    print(result)
```

通过代码可以直观的看出，KNN 算法没有进行数据训练的过程，而是直接使用待判定数据与已知数据进行比较计算进而得出结果。所以 KNN算法不具有显式的学习过程。

##### 欧氏距离

上述实例中列举的数据只有两个维度，当数据拓展到多维时，使用的公式就是`欧氏距离(欧几里得度量)`。上面使用的两点间距离公式其实就是欧氏距离在二维空间上的情况，代码还是一样的。

![knn1.jpg](https://s.im5i.com/2021/01/11/knn1.jpg)

<br/>

### 手写数字分类

介绍完KNN算法的原理和实现，下面实战一个很简单的实例项目。这里直接使用`sklearn`库完成。

##### 数据集

将原图像为`32像素x32像素的手写数字图像`转化为文本格式存储，有笔画的部分用数字`1`表示，空白区域用数字`0`表示。示例图像如下，三个文件对应的数字分别是0、6、8：

![knn2.jpg](https://s.im5i.com/2021/01/11/knn2.jpg)

这些文本格式存储的数据集命名规则为：数字的值_该数字的样本序号。

![knn3.jpg](https://s.im5i.com/2021/01/11/knn3.jpg)

整理好的数据集下载地址：[数据集下载地址](https://github.com/VixeruntR/MyData/tree/master/KNN)

##### sklearn实现KNN算法

`sklearn.neighbors`模块实现了相关算法，包含分类和回归方法。这里使用`KNeighborsClassifier`方法即可实现 KNN分类，主要参数如下：

+ n_neighbors：KNN 的K值，默认为5
+ weights：可选`uniform`或`distance`或自定义函数。uniform表示权重均等，即所有邻近点的权重都是相等的；distance表示权重不均等，距离近的点比距离远的点的权重大；自定义函数接收一个表示距离的数组，返回一组维数相同的权重。
+ algorithm：快速K近邻搜索算法，默认为`auto`，即算法自己决定合适的搜索算法。除此之外，也可以自己指定`ball_tree、kd_tree、brute`这些搜索算法。
+ p：距离度量公式。默认为2，即使用欧式距离进行距离度量。设置为1时使用曼哈顿距离进行距离度量。
+ n_jobs：并行处理参数，默认为`None`，表示使用单进程；如果为`-1`会使用CPU所有核心用于并行工作。

##### 完整代码

其中`image2vector`方法是将32x32形式存储在文本中的数据转化为1x1024的向量；`train_data_init`方法是构造训练数据矩阵和标签列表。

```python
import numpy as np
import os
from sklearn.neighbors import KNeighborsClassifier

def image2vector(file_path):
    vector_data = list()
    with open(file_path) as f:
        line_data = f.readlines()
        for data in line_data:
            vector_data += [float(i) for i in data.strip()]
    return np.array([vector_data])

def train_data_init(train_data_path):
    train_labels = []
    train_files = os.listdir(train_data_path)
    train_file_num = len(train_files)
    train_data_mat = np.zeros((train_file_num, 1024))

    for i in range(train_file_num):
        current_file_name = train_files[i]
        current_file_label = int(current_file_name.split('_')[0])
        train_labels.append(current_file_label)
        current_file_data = image2vector(
            os.path.join(train_data_path, current_file_name))
        train_data_mat[i, :] = current_file_data
    return train_data_mat, train_labels

def test_data(model, test_data_path):
    TruePrediction = 0
    test_files = os.listdir(test_data_path)
    test_file_num = len(test_files)
    for i in range(test_file_num):
        test_file_name = test_files[i]
        test_file_label = int(test_file_name.split('_')[0])
        test_file_data = image2vector(
            os.path.join(test_data_path, test_file_name))
        predict_result = model.predict(test_file_data)
        if predict_result == test_file_label:
            TruePrediction += 1
    print('一共有 %d 组测试数据' % test_file_num)
    print("预测正确的数据有 %d 组" % TruePrediction)
    print("正确率: %.2f%%" % (round(TruePrediction / test_file_num, 4) * 100))

def main(train_root, test_root):
    train_data_mat, train_labels = train_data_init(train_root)
    knn = KNeighborsClassifier(n_neighbors=5, algorithm='auto')
    knn.fit(train_data_mat, train_labels)
    test_data(knn, test_root)

main('trainingDigits', 'testDigits')
-----------------------------------------------------------------------
> 一共有 946 组测试数据
> 预测正确的数据有 928 组
> 正确率: 98.10%
```

根据测试结果可以看到，分类的正确率还是很高的。当然这里使用的都是默认的参数，实际调试时还需要根据数据集调整`KNeighborsClassifier`方法的各项参数以达到所需的目标。