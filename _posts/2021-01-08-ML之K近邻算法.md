---
layout:     post   				    
title:      K近邻算法 				
subtitle:    
date:       2021-01-08 				
author:     WarmStar 						
header-img: img/blog12.jpg 	
catalog: true 				
tags:							
    - ML
---

### 简述

K近邻算法(K-nearest neighbor, KNN)是一种**基本分类与回归方法**。

工作原理：有一个样本集，且样本集中的每一组数据都有一个标签，即已知每一组数据与其所属分类的对应关系。输入没有标签的新数据后，会将新数据的每个特征与样本集中的数据的对应特征相比较，然后提取样本集中最相似数据的分类标签。一般只选择样本集中`前K个`最相似的数据，这也就是K近邻算法中K的含义。最后选择这 K 个最相似数据中**出现次数最多**的标签作为新数据的类别。

##### 算法流程

+ 计算测试数据与各个训练数据之间的距离
+ 按照距离的递增关系进行排序
+ 选取距离最小的 K 个点
+ 确定前 K 个点所在类别的出现频率
+ 返回前 K 个点中出现频次最高的类别作为测试数据的预测分类

<br/>

### 简单实例

##### 简单样本集

使用 KNN算法分类一组简单的数据，根据玩过游戏种类的数量区分人物性别。

| 样本 | 换装 | 格斗 | 性别 |
| :--: | :--: | :--: | :--: |
|  A   |  2   |  40  |  男  |
|  B   |  4   |  35  |  男  |
|  C   |  47  |  3   |  女  |
|  D   |  49  |  1   |  女  |

上表就是一个简单的数据集，只有两个特征。根据一般人的认知经验来看，玩过换装游戏多的人大概率是女性，换过格斗游戏多的人大概率是男性。这时候如果再给出一组新数据：换装游戏玩过1款，格斗游戏玩过36款，人和KNN算法都可以给出这个人是男性的答案。但如果给出的数据是换装游戏玩过32款，格斗游戏玩过30款，人就不好判断了，但KNN不会纠结什么，它会根据**既定的规则**给出计算得到的结果，虽然这个结果未必与现实相符。

##### 距离度量

上面提到KNN会根据既定的规则来比较特征进行分类，这里就介绍一下这个既定的规则：**距离度量**，即将量化后的特征值代入距离计算公式计算距离。这里的例子中只有两种特征，那么用二维实数向量空间的坐标就可以表示了，距离计算公式为**对应坐标差的平方和再开根号**：

```python
|D| = [(x - x_)^2 + (y - y_)^2] ^ (0.5)
```

根据上文给出的**算法流程**计算新数据 (1, 36) 的类别，这里 K 取 3：

1. 分别计算样本集中各个数据的距离得到：4.12、3.16、56.61、59.4
2. 按增序排列后得到：3.16、4.12、56.61、59.4
3. 选取距离最近的 3 个点得到：3.16、4.12、56.61
4. 前3个点所属类别出现的频次为：男出现2次、女出现1次
5. 由于男出现频次最高，所以将(1, 36) 分类为男

以上就是一个简单KNN算法计算全过程。注意如果`K值取1`，这个算法就变成了`最近邻算法`，而非`K近邻算法`。

<br/>

### 手撕 KNN

为了加深理解KNN的原理和实现细节，下面用Python的基本库实现该算法。

##### 生成数据集

用`numpy`生成数据集矩阵，标签则用一个一维列表存储即可。数据形式参考如下输出。

```python
import numpy as np

def dataset():
    data = np.array([[2, 40], [4, 35], [47, 3], [49, 1]])
    label = ['男', '男', '女', '女']
    return data, label

if __name__ == '__main__':
    data, label = dataset_init()
    print(data)
    print(label)
-----------------------------------------------------------------------
>  [[ 2 40]
 	[ 4 35]
 	[47  3]
 	[49  1]]
>  ['男', '男', '女', '女']
```

##### KNN算法

使用 numpy 可以非常方便的将列表数据转化成矩阵，然后一次矩阵运算即可得到所有距离值。该算法代码实现非常简单，可参考如下示例。

```python
def dataset():
    data_ = np.array([[2, 40], [4, 35], [47, 3], [49, 1]])
    labels_ = ['男', '男', '女', '女']
    return data_, labels_

def knn(train_data, train_labels, test_data, k):
    # param train_data: 训练集(array type)
    
    # param train_labels: 训练集标签(list type)
    
    # param test_data: 需要分类的数据(list type)
    
    # param k: KNN算法的K值(int type)
    
    test_data_expand = np.tile(test_data, (train_data.shape[0], 1))
    EuclideanDistance = (((test_data_expand - train_data) ** 2).sum(axis=1)) ** 0.5
    sorted_index = EuclideanDistance.argsort()

    label_count = dict()
    for i in range(k):
        target_label = train_labels[sorted_index[i]]
        label_count[target_label] = label_count.get(target_label, 0) + 1

    label_count = sorted(label_count.items(), 
                         key=lambda x: x[1], 
                         reverse=True)
    return label_count[0][0]


if __name__ == '__main__':
    data, labels = dataset()
    result = knn(data, labels, [1, 36], 3)
    print(result)

```

通过代码可以直观的看出，KNN 算法没有进行数据训练的过程，而是直接使用待判定数据与已知数据进行比较计算进而得出结果。所以 KNN算法不具有显式的学习过程。

##### 欧氏距离

上述实例中列举的数据只有两个维度，当数据拓展到多维时，使用的公式就是`欧氏距离(欧几里得度量)`。上面使用的两点间距离公式其实就是欧氏距离在二维空间上的情况，代码还是一样的。

![knn1.jpg](https://s.im5i.com/2021/01/11/knn1.jpg)